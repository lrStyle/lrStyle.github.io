
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Ren Li&#39;s blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Ren Li&#39;s blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Ren Li&#39;s blog">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Ren Li&#39;s blog">
  
    <link rel="alternative" href="/atom.xml" title="Ren Li&#39;s blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" href="/css/style.css">
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
</head>
<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Ren Li&#39;s blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Think and write down</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" class="search-form">
          <input type="search" name="word" maxlength="20" class="search-form-input" placeholder="Search">
          <input type="submit" value="" class="search-form-submit">
          <input name=tn type=hidden value="bds">
          <input name=cl type=hidden value="3">
          <input name=ct type=hidden value="2097152">
          <input type="hidden" name="si" value="yoursite.com">
        </form>
      </div>
    </div>
  </div>
</header>
    <div class="outer">
      <section id="main">
  
    <article id="post-Python/numpy笔记" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/25/Python/numpy笔记/" class="article-date">
  <time datetime="2018-12-25T17:11:34.447Z" itemprop="datePublished">2018-12-25</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/25/Python/numpy笔记/">numpy笔记</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <ul>
<li><p>基本类型: np.array<br>一维: <code>np.array([1,2])</code><br>二维: <code>np.array([[1,2], [3,4]])</code><br>三维: <code>np.array([[[1,2], [3,4]], [[5,6], [7,8]]])</code><br>四维: <code>np.array([[[[1,2], [3,4]], [[5,6], [7,8]]], [[[1,2], [3,4]], [[5,6], [7,8]]]])</code><br>把维度的增加理解为向高一级的抽象(四维数组就是多个三维数组的组合).</p>
</li>
<li><p>broadcasting<br>eg: <code>2*[1,2,3] = [2, 4, 6]</code><br>一种节省内存的方法, 让系统自动推断矩阵的shape(不用定义两个完全一样的shape, 相同的元素可以由向量省略为一个数)<br>详细文档: <a href="https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html" target="_blank" rel="noopener">https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html</a></p>
</li>
<li><p>行向量-&gt;列向量:<br>普通矩阵的转置直接<code>.T</code>即可, 但<code>.T</code>不适用行向量&amp;列向量, 因此需要其他方法: <code>reshape(-1, 1)</code>,只指定列宽为1, -1表示让系统自动推断大小.</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://yoursite.com/2018/12/25/Python/numpy笔记/" data-id="cjqqhv6j2002m3hqxg9ilnuoh" class="article-share-link" data-share="baidu" data-title="numpy笔记">Share</a>
      

      
        <a href="http://yoursite.com/2018/12/25/Python/numpy笔记/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-cs224n/lecture 3" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/26/cs224n/lecture 3/" class="article-date">
  <time datetime="2018-11-26T22:55:12.828Z" itemprop="datePublished">2018-11-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/11/26/cs224n/lecture 3/">lecture 2_Morew Word Vectors</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="单词的向量表征"><a href="#单词的向量表征" class="headerlink" title="单词的向量表征"></a>单词的向量表征</h2><p>本节lecture主要是对上一节内容的扩展，重点还是在如何用向量来表征单词上，以使得单词间获得更好的相似性，而且我们可以直观地理解这种相似性。<br>核心思想是通过计算单词间的<strong>共现</strong>（co-occurence）次数/频率/概率来表征单词间的相似性——两个单词同时出现在相同context中的次数多了，我们就认为这两个单词比较相似。</p>
<ul>
<li>一种模型是skip-gram模型，一步步地计算每个窗口，最终得出单词间相似性。</li>
<li>另一种模型是Glove，基于共现矩阵的思想，一次性统计语料库中所有单词两两间的共现次数，从而得出单词间的相似性。</li>
</ul>
<h2 id="如何评估词向量的优劣"><a href="#如何评估词向量的优劣" class="headerlink" title="如何评估词向量的优劣"></a>如何评估词向量的优劣</h2><p>两种评价思路：intrinsic &amp; extrinsic（内部评价和外部评价）</p>
<ul>
<li><p>intrinsic evaluation<br>通过分析词向量本身的一些特性来评价：如词与词之间的余弦相似度、欧式距离或类比关系（analogy, 即 man-woman=king-queen 这种线性关系），如果得出的词向量这些关系好（eg. 词义相似的词向量在向量空间中距离也很近），那么这个词向量就好。<br>注：总的来说<strong>Glove</strong>表现的最好。</p>
</li>
<li><p>extrinsic evaluation<br>通过一些下游的任务来评价词向量的好坏，如我们把训练好的词向量用于命名实体识别任务中，看词向量实际表现的怎么样。<br>注：还是<strong>Glove</strong>模型表现的最好。</p>
</li>
<li><p>两种评价方法的优劣<br>intrinsic evaluation速度快，可以帮助我们更好地理解这个任务本身。但实际上这并不是一种真实的评价方法，即我们不知道这种词向量模型在实际任务中到底表现得怎么样。<br>extrinsic evaluation则是一种真实的评价方法，可以确切地得出词向量模型的好坏。但缺点是计算出最终结果很费时间，而且如果表现不好，我们并不能准确地分析出原因（到底是向量模型本身的原因，还是向量模型和其他模型互相影响(interact)的原因）</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://yoursite.com/2018/11/26/cs224n/lecture 3/" data-id="cjqqhv6j6002w3hqxmufhq7uw" class="article-share-link" data-share="baidu" data-title="lecture 2_Morew Word Vectors">Share</a>
      

      
        <a href="http://yoursite.com/2018/11/26/cs224n/lecture 3/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cs224n/">cs224n</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-cs224n/background" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/06/cs224n/background/" class="article-date">
  <time datetime="2018-11-06T21:07:19.846Z" itemprop="datePublished">2018-11-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/11/06/cs224n/background/">基础知识</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="梯度下降（gradient-decent）"><a href="#梯度下降（gradient-decent）" class="headerlink" title="梯度下降（gradient decent）"></a>梯度下降（gradient decent）</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script></p>
<ul>
<li>什么是梯度：在多元函数中，梯度可理解为求各个变量的偏导数，最终各个偏导组合成梯度向量，即代表该函数在该点变化最快的方向。<br>eg: \(f(x,y)=x^2+y^2\), 梯度向量为(2x,2y)，在点(1,1)处的梯度即为(2,2)，即沿(2,2)这个方向函数变化最快。</li>
</ul>
<h3 id="Vanilla-Gradient-Descent（普通梯度下降）"><a href="#Vanilla-Gradient-Descent（普通梯度下降）" class="headerlink" title="Vanilla Gradient Descent（普通梯度下降）"></a>Vanilla Gradient Descent（普通梯度下降）</h3><p>1、求解梯度向量<br>2、一点点沿着梯度的方向迭代更新函数值，使函数最终下降到局部最小值处。<br>$$<br>\theta^{new}_j=\theta^{old}-\alpha\nabla_{\theta}J(\theta)=\theta^{old}_j-\alpha\frac {\partial}{\partial \theta^{old}_j}J(\theta)<br>$$<br>注：\(\alpha\)是步长（setp size），步长太小求解速度慢，步长太大则会造成抖动。<br>解决方法：距离谷底较远时，步幅大些比较好（加快速度）；接近谷底时，步幅小些比较好（以免跨过界）。距离谷底的远近可以通过梯度的数值大小间接反映，接近谷底时，坡度会减小，因此可设置步长与梯度数值大小正相关。</p>
<h3 id="Stochastic-Gradient-Descent（SGD，随机梯度下降）"><a href="#Stochastic-Gradient-Descent（SGD，随机梯度下降）" class="headerlink" title="Stochastic Gradient Descent（SGD，随机梯度下降）"></a>Stochastic Gradient Descent（SGD，随机梯度下降）</h3><p>VGD中，损失函数相当于每个数据样本取平均值，因此每次更新都需要遍历所有data，当数据量太大，更新一次梯度会花费大量时间，因此并不可行。<br>解决这个问题的基本思路：只通过一个随机选取的数据(xn,yn)来获取梯度（通常损失函数都是很多项的变量加和得到的，这时只取一项计算其梯度，用来估计整体的梯度），这种方法叫随即梯度下降。<br>虽然这样估计梯度非常粗糙，但事实证明这种方法效果还不错。</p>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://yoursite.com/2018/11/06/cs224n/background/" data-id="cjqqhv6j4002s3hqxnicghj1j" class="article-share-link" data-share="baidu" data-title="基础知识">Share</a>
      

      
        <a href="http://yoursite.com/2018/11/06/cs224n/background/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cs224n/">cs224n</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-cs224n/lecture 2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/05/cs224n/lecture 2/" class="article-date">
  <time datetime="2018-11-05T21:32:18.948Z" itemprop="datePublished">2018-11-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/11/05/cs224n/lecture 2/">lecture 2_Word Vectors</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script>

<h2 id="How-to-represent-the-meaning-of-a-word-in-computer"><a href="#How-to-represent-the-meaning-of-a-word-in-computer" class="headerlink" title="How to represent the meaning of a word in computer"></a>How to represent the meaning of a word in computer</h2><h3 id="离散表示（discrete-representation）"><a href="#离散表示（discrete-representation）" class="headerlink" title="离散表示（discrete representation）"></a>离散表示（discrete representation）</h3><ul>
<li>把单词作为一个个原子符号（atomic symbol）来表征：hotel, conference, walk…</li>
<li>对应到计算机中，使用<strong>独热编码（one-hot encoding）</strong>来存储：<code>[0 0 0 0 1 0 0 0]</code></li>
<li>存在的问题：<br>1、向量规模过大：每个单词都对应一个单独的位，若要存储所有单词，则会需要一个非常大的向量。<br>2、难以计算单词间的相似性：这是一种localist representation, 每个one-hot representation是独立存在的，no inherent notion of similarity, 两两间并没有天然的相似性关系。<br>3、因此与其去研究an approach to work out similarity relationship between one-hot representations，不如直接探究<strong>an approach where representation of word encodes its meaning inherently</strong>，这样就能直观去计算单词间的similarity，这就是分布表示（distributed representation）的设计初衷。<br>注：这里的similarity指的是单词词义上的相似，而不是结构相似。</li>
</ul>
<h3 id="分布表示（distributed-representation）"><a href="#分布表示（distributed-representation）" class="headerlink" title="分布表示（distributed representation）"></a>分布表示（distributed representation）</h3><ul>
<li><strong>核心思想：</strong>通过单词的上下文（context）来理解词意。如<code>banking</code>这个单词，我们让计算机知道经常和它一起出现的其他单词，就相当于明白了单词的用法——怎样将单词放到正确的上下文中，就相当于理解了单词的meaning。<br><img src="/images/cs224n/2_distributed representation.png" width="500" height="120" align="center"></li>
<li><strong>具体方法：</strong>用向量定义词语的含义。通过调整一个单词及其上下文单词的向量，使得根据两个向量可以推测两个单词词义的相似度，就可以根据中心词向量预测上下文/根据上下文向量预测中心词。这就是我们常说的<strong>word2vec</strong><br>遵循的基本原则：出现在相同上下文中的单词词义会相似。</li>
<li>注：区分distributed和distributional<br>distributed meaning：一种词义表示，和one-hot相对，one-hot将词义独立地存储在本地（<code>[0 0 0 0 1 0 0 0]</code>向量中），distributed则存储在一个大的稠密的向量空间中。<br>distributional similarity：一种通过上下文来理解词义的方法，与denotational相对。<br>We use distributional similarity to <strong>build</strong> distributed meaning.</li>
</ul>
<h2 id="word2vec模型"><a href="#word2vec模型" class="headerlink" title="word2vec模型"></a>word2vec模型</h2><h3 id="Skip-grams-SG"><a href="#Skip-grams-SG" class="headerlink" title="Skip-grams(SG)"></a>Skip-grams(SG)</h3><p><img src="/images/cs224n/2_Skipgram_prediction.png" width="300" height="150" alt="Skipgram_prediction" align="center/"><br><img src="/images/cs224n/2_Skipgram_model.png" width="450" height="300" alt="Skipgram_model" align="center/"></p>
<h4 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h4><p>通过中心词预测上下文。</p>
<h4 id="定义预测单词上下文的模型"><a href="#定义预测单词上下文的模型" class="headerlink" title="定义预测单词上下文的模型"></a>定义预测单词上下文的模型</h4><p><strong>预测模型：</strong>\(p(context|w_t)\)，表示在给定中心词\(w_t\)的条件下，正确预测出上下文单词的概率。<br><strong>再具体点：</strong>\(p(w_{t-1}|w_t)\)，\(p(w_{t-2}|w_t)\)，\(p(w_{t+1}|w_t)\)，\(t, t+1, t-1\)都是表示单词在文中出现的序号。<br><strong>最终形式：</strong><br>$$ p(w_{t+j}|w_t)=p(o|c)=\frac {exp(u_0^Tv_c)} {\sum_{w=1}^v exp(u_w^Tv_c)} $$<br>\(t, t+j\)是单词在文中的位置.<br>\(o, c\)则是单词在单词表中的序号，相当于在更为一般地表征两个单词的关联（与具体文本无关）。<br>\(v_c\)是中心词的词向量，\(u_0\)是上下文单词的词向量。<br><strong>公式理解：</strong>首先是\(u_0^Tv_c\)，这是一个点积操作，可用来粗糙地衡量单词间相似性（单词越相似，向量值越相近，乘积越大）；其次是softmax操作，用来把值转换成概率：<br>$$\frac {exp(u_0^Tv_c)} {\sum_{w=1}^v exp(u_w^Tv_c)} $$<br>最终即得出了由中心词预测上下文单词的概率。</p>
<h4 id="定义损失函数（Loss-Function）"><a href="#定义损失函数（Loss-Function）" class="headerlink" title="定义损失函数（Loss Function）"></a>定义损失函数（Loss Function）</h4><p>1、首先对模型全部相乘，表示文本整体的预测正确率：<br>$$ J^`(\theta)=\prod_{t=1}^T \prod_<br>{ \begin{align}<br>-m\le j \le m\\<br>j \neq 0<br>\end{align} }<br>p(w_{t+j}|w_t)$$<br>目标：整体正确率最大，maximize the function，<br>2、做Negtive Log Likelihood处理，使maximize-&gt;minimize，即得到最终的损失函数。<br>$$ J(\theta)=-\frac {1}{T} \sum_{t=1}^T \sum_<br>{ \begin{align}<br>-m&amp; \le j \le m\\<br>j \neq 0<br>\end{align} }<br>p(w_{t+j}|w_t)\\<br>p(w_{t+j}|w_t)=\frac {exp(u_0^Tv_c)} {\sum_{w=1}^v exp(u_w^Tv_c)}<br>$$<br>最终目标：使损失函数最小。</p>
<h4 id="怎样求损失函数最小值：梯度下降（gradient-descent）"><a href="#怎样求损失函数最小值：梯度下降（gradient-descent）" class="headerlink" title="怎样求损失函数最小值：梯度下降（gradient descent）"></a>怎样求损失函数最小值：梯度下降（gradient descent）</h4><p>注1：具体求解过程见lecture slide。<br>注2：求梯度时对\(u_o和v_c\)都要求偏导，最后组合成梯度向量。且求的是向量的导数，和实数求偏导有一定区别。<br>注3：当数据量过大时，可使用SGD随机梯度下降来加快速度（一个窗口计算一次梯度，用来估计整体梯度）。</p>
<h4 id="怎样训练模型"><a href="#怎样训练模型" class="headerlink" title="怎样训练模型"></a>怎样训练模型</h4><p>模型中的参数就是每个单词对应的word vector向量，将所有参数组合成一个的大向量\(\theta\)，即作为损失函数的自变量。<br><img src="/images/cs224n/2_theta.png" width="150" height="150" alt="theta举例（里面单词是随便举的例子）" align="center"><br>总共V个单词，每个单词对应两个词向量，词向量每个d维，因此\(\theta\)总长度为2dV。<br>为何每个单词对应两个词向量：作为中心词时一个，作为上下文单词时一个，这样更方便数学处理。</p>
<h3 id="Continuous-Bag-of-Words-CBOW"><a href="#Continuous-Bag-of-Words-CBOW" class="headerlink" title="Continuous Bag of Words(CBOW)"></a>Continuous Bag of Words(CBOW)</h3><h4 id="作用-1"><a href="#作用-1" class="headerlink" title="作用"></a>作用</h4><p>通过上下文预测中心词<br>To be continued</p>
<h2 id="遗留问题："><a href="#遗留问题：" class="headerlink" title="遗留问题："></a>遗留问题：</h2><ol>
<li>SG模型中，计算每个上下文词向量的值时，context matrix是一样的？ 那计算出的结果不就一样了？————context matrix是一样的，但因为每个单词的词向量表征是不一样的，所以最终训练出来的上下文词向量也是不一样的。直观来说，context martrix是一各由中心词-&gt;上下文的转换, 在词向量空间中从一个单词向这个单词周围来扩展(词义相近单词离得近), 这种变化对每个单词都是一样的, 因此context martrix一样也是正常的.</li>
<li>如何迭代\(\theta\)来更新梯度？把\(\theta\)初始化成什么样子？<br>————按照梯度下降的法则来更新参数; 参数初始化有多种策略.</li>
<li>这是有监督的学习过程，损失函数中没有出现计算误差的部分？</li>
</ol>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://yoursite.com/2018/11/05/cs224n/lecture 2/" data-id="cjqqhv6j800303hqxzqdvjbhx" class="article-share-link" data-share="baidu" data-title="lecture 2_Word Vectors">Share</a>
      

      
        <a href="http://yoursite.com/2018/11/05/cs224n/lecture 2/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cs224n/">cs224n</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-cs224n/lecture 1" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/02/cs224n/lecture 1/" class="article-date">
  <time datetime="2018-11-02T12:33:38.275Z" itemprop="datePublished">2018-11-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/11/02/cs224n/lecture 1/">lecture 1_Introduction</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="关于人类语言-human-language"><a href="#关于人类语言-human-language" class="headerlink" title="关于人类语言(human language)"></a>关于人类语言(human language)</h2><h3 id="人类语言的特点"><a href="#人类语言的特点" class="headerlink" title="人类语言的特点"></a>人类语言的特点</h3><ul>
<li><strong>语言就是符号：</strong>人类语言本质是一个符号系统（symbol system），无论是汉字还是英文字母，都是一种符号，用来承载、传递我们想要表达的意思（meaning）。</li>
<li><strong>语言的载体：</strong>sound, vision(writting), gesture，不论是哪一种载体，都是一种连续的交流方式。</li>
<li><strong>大脑是一种符号处理器</strong>（symbolic processors）：我们可以把大脑处理语言看成是<strong>连续模式的激活过程</strong>（continious pattern of activation）。</li>
<li>因此我们可以得到启发：探索一种<strong>连续的编码模式</strong>来表达思想(explore a continous encoding patten of thought)。这也是很多NLP算法的处理思想，同时也解决了sparsity的问题。</li>
</ul>
<h2 id="关于NLP"><a href="#关于NLP" class="headerlink" title="关于NLP"></a>关于NLP</h2><h3 id="NLP-levels"><a href="#NLP-levels" class="headerlink" title="NLP levels"></a>NLP levels</h3><p><img src="/images/cs224n/1_NLP_levels.png" width="500" height="270" alt="NLP levels" align="center"></p>
<ul>
<li><strong>两大来源：</strong>通过语音或者文本。语音：语音分析（phonetic）或音韵分析（phonological）；文本：OCR识别（Optical Character Recognition，光学字符识别）或分词处理（tokenization）。通过上述方法来获取NLP的输入。</li>
<li><strong>形态分析</strong>（morphological）：对单词进行形态分析：前缀（prefix）、后缀（suffix）等。</li>
<li><strong>句法分析</strong>（syntactic）：分析句子结构、语法结构（structure of sentence）。</li>
<li><strong>语义理解</strong>（semantic interpretation）：work out the meaning of sentences.</li>
<li><strong>语篇处理</strong>（discourse processing）：因为大多数句子含义需要通过上下文（context）来推测，不能仅仅只分析当前句子，因此就有了the field of discourse processing。<br>注：cs224n课只重点讲syntatic &amp; semantic analysis 这两块，以及一部分speech signal analysis。</li>
</ul>
<h3 id="NLP-Applications"><a href="#NLP-Applications" class="headerlink" title="NLP Applications"></a>NLP Applications</h3><ul>
<li>较低级：spell checking, keyword search, finding synonyms</li>
<li>中级：extracting information。个人比较感兴趣的方向，让计算机可以阅读文本，理解在讲些什么，至少知道讲的是哪方面内容；从文本中识别、抽取某方面内容；或者为文本阅读难度分级（work out the reading level of school text）,识别文本的目标受众（intended audience of document）；情感分析（positive or negetive）。</li>
<li>高级：机器翻译、对话机器人、智能问答、机器撰写（exploit the knowledge of world）</li>
</ul>
<h3 id="Why-is-NLP-hard"><a href="#Why-is-NLP-hard" class="headerlink" title="Why is NLP hard"></a>Why is NLP hard</h3><ul>
<li><strong>语言本身的困难性</strong>：Ambiguilty of language, and moreover, humen always do not say everything（为了高效表达，语言使用中会出现很多省略）.</li>
<li><strong>表征语言很困难</strong>：Complexity of representing, using linguistic/situational/world knowledge.</li>
<li><strong>解释语言很困难</strong>：Real meaning of the language depends on real world, common sense, and contextual knowledge.</li>
</ul>
<h2 id="关于deep-learning"><a href="#关于deep-learning" class="headerlink" title="关于deep learning"></a>关于deep learning</h2><h3 id="传统机器学习的问题"><a href="#传统机器学习的问题" class="headerlink" title="传统机器学习的问题"></a>传统机器学习的问题</h3><ul>
<li>Most traditional machine learning algorithms work well because of human-designed representations and input featured.</li>
<li>“Machines” are only used to optimize weights that best make a final prdiction. </li>
<li>Moreover, manually designed featured are often over-specified(lack of generalization), incomplete and take a long time to design and validate.</li>
</ul>
<h3 id="What-is-deep-learning"><a href="#What-is-deep-learning" class="headerlink" title="What is deep learning"></a>What is deep learning</h3><ul>
<li>Subfield of machine learning and part of representation learning.</li>
<li>Deep learning algorithms attempt to learn (multiple levels of) representations and an output themselves. </li>
<li>We only input the raw data.</li>
<li>In a lot of times, deep learning means neural networks (the dominant model family).</li>
</ul>
<h3 id="deep-learning-in-NLP"><a href="#deep-learning-in-NLP" class="headerlink" title="deep learning in NLP"></a>deep learning in NLP</h3><p>核心思想：用vector去表征语言，用神经网络去组织、计算vector。</p>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://yoursite.com/2018/11/02/cs224n/lecture 1/" data-id="cjqqhv6j5002u3hqxt5njws2n" class="article-share-link" data-share="baidu" data-title="lecture 1_Introduction">Share</a>
      

      
        <a href="http://yoursite.com/2018/11/02/cs224n/lecture 1/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cs224n/">cs224n</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-uwtsd_modules/Distributed &amp; Cluster Computing" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/10/18/uwtsd_modules/Distributed & Cluster Computing/" class="article-date">
  <time datetime="2018-10-18T21:29:42.408Z" itemprop="datePublished">2018-10-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/10/18/uwtsd_modules/Distributed & Cluster Computing/">Distributed &amp; Cluster Computing</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="Interfaces"><a href="#Interfaces" class="headerlink" title="Interfaces"></a>Interfaces</h2><ul>
<li>Interface-Based Programming<br>An interface only defines a signature of properties and methods(method name, type of each parameter, type of return value).<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">interface</span> <span class="title">IMyInterface</span></span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> property1</span><br><span class="line">    &#123;</span><br><span class="line">        get;</span><br><span class="line">        set;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Method1</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Method2</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>A class is used to implement the interface by providing the actual code for those methods.</p>
<ul>
<li>Interface Name Form<br>Microsoft dictates that all interface names start with the <code>I</code> characters, that they not include underscore character, and that they use Pascal casing when the name contains mutiple words(first letter of each word is uppercase). </li>
</ul>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://yoursite.com/2018/10/18/uwtsd_modules/Distributed & Cluster Computing/" data-id="cjqqhv6j7002y3hqxy4h9jhon" class="article-share-link" data-share="baidu" data-title="Distributed &amp; Cluster Computing">Share</a>
      

      
        <a href="http://yoursite.com/2018/10/18/uwtsd_modules/Distributed & Cluster Computing/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/uwtsd-modules/">uwtsd_modules</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Paper Notes/Efficient Estimation of Word Representations in Vector Space" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/09/13/Paper Notes/Efficient Estimation of Word Representations in Vector Space/" class="article-date">
  <time datetime="2018-09-13T07:49:59.000Z" itemprop="datePublished">2018-09-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/09/13/Paper Notes/Efficient Estimation of Word Representations in Vector Space/">Notes of Efficient Estimation of Word Representations in Vector Space读书笔记</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p><a href="https://arxiv.org/pdf/1301.3781.pdf?" target="_blank" rel="noopener">论文地址点这里</a></p>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script> 

<h2 id="论文内容概述"><a href="#论文内容概述" class="headerlink" title="论文内容概述"></a>论文内容概述</h2><p>这是谷歌发布的关于词向量的经典论文，针对传统模型无法表征单词间关联性的问题，本文提出了两种连续的词向量模型，并针对模型实现了分布式学习模块，最后基于词向量的线性运算设计了一种评价模型精度的方法，取得了良好的效果。</p>
<h3 id="简介-INTRODUCTION"><a href="#简介-INTRODUCTION" class="headerlink" title="简介(INTRODUCTION)"></a>简介(INTRODUCTION)</h3><ul>
<li><p><strong>传统NLP技术的特点</strong><br>传统NLP技术主要是基于单词间没有相似性的思想，将词汇以单词表索引的形式来表示。这样做虽然有很多好处，如简单、健壮、使用大量数据训练的简单模型的效果好于使用少量数据训练的复杂模型等；但同样也有很多限制，如语料数据库没有足够的数据、无法表征单词间的关联等。因此作者认为原先的技术很难再取得突破，需要去发掘新的技术。</p>
</li>
<li><p><strong>论文主要工作</strong><br>1、从大量单词和短语数据中学习出高质量的词向量，并且要保持一个合适的向量维度。<br>2、使用了一种评估词向量精度的技术，期望不仅相似的单词可以趋向接近，同时单词间能具有更多维的相似度。<br>3、使用单词偏移技术对词向量做简单的代数运算，以更好地挖掘词向量间的相似性规则。<br>4、设计了一个综合测试集来探究句法和语义的规律性，并研究了词向量维度和训练数据量这两个因素对训练时间和模型精度的影响。</p>
</li>
</ul>
<h3 id="模型框架-Model-Architectures"><a href="#模型框架-Model-Architectures" class="headerlink" title="模型框架(Model Architectures)"></a>模型框架(Model Architectures)</h3><p>在本节中，作者使用了NNLM和RNNLM两种神经网络来学习连续的单词表示方法，并且对不同神经网络的训练复杂度进行了分析。</p>
<ul>
<li><p><strong>训练复杂度公式</strong><br>作者首先提出了分析训练复杂度的公式：\(O=E\times T\times Q\)，其中E为训练的趟数，T为训练集中单词的数量，Q为表征模型复杂度的变量，下文还会对Q具体分析。</p>
</li>
<li><p><strong>前馈神经网络语言模型(NNLM)</strong><br>基本结构：整个网络分为输入层、映射层、隐含层和输出层，输入层将前N个单词编码为1-of-V的向量，V为词表大小，这N个单词共享相同的映射矩阵，阵映射到映射层，之后再从映射层转换到隐含层，经过一系列计算后输出最终结果。<br>模型复杂度：\(Q=N\times D+N\times D\times H+H\times V\)，因为N是一个较小的量，因此\(N\times D\)也比较小；另外对单词表用二叉树做优化，可使得\(V\)下降到\(log_2(V)\)，这样\(H\times V\)也会减小，最终模型的复杂度主要就集中在\(N\times D\times H\)，即映射层-&gt;隐含层这部分。<br>注：对于单词表的表示，还可以继续用哈夫曼树做优化，减少高频单词的编码长度，从而进一步降低输出层的计算量。</p>
</li>
<li><p><strong>循环神经网络语言模型(RNNLM)</strong><br>模型特点：RNN最大的特点是其自连接性，即状态的更新不仅取决于输入，还取决于上一时刻自己本身的状态，这个特性就使得RNN具有了一种“短时”的记忆，可以更好地表示前后文本之间的关联。<br>模型复杂度：RNNLM中没有映射层，因此模型复杂度简化为\(Q=H\times H+H\times V\)。和上文同理，\(V\)可以优化为\(log_2(V)\)，因此模型的复杂度主要集中在\(H\times H\)。</p>
</li>
<li><p><strong>神经网络的平行训练模型</strong><br>为了提升在大规模数据集上训练模型的效率，作者在DistBelief框架的基础上实现了一个分布式计算模型，允许多个副本并行地学习训练，而且所有副本会同步进行梯度更新，作者在下文会分析模型的具体效果。  </p>
</li>
</ul>
<h3 id="新的对数线性模型-New-Log-linear-Models"><a href="#新的对数线性模型-New-Log-linear-Models" class="headerlink" title="新的对数线性模型(New Log-linear Models)"></a>新的对数线性模型(New Log-linear Models)</h3><p>在本节中，作者提出了两个连续词向量模型，希望能使用一种更简单的模型来训练更大量的数据，以通过增加训练量来提升训练效果。</p>
<ul>
<li><p><strong>连续词袋模型(Continuous Bag-of-Words Model)</strong><br>基本结构：模型与前馈NNLM比较相似，但是去掉了非线性隐含层，并且所有单词直接共享整个映射层，而非只是映射矩阵。通过这种方法，所有的单词被映射到同一位置，对这些向量进行平均后即为最终的映射结果。<br>模型特点：1、所有的单词最终都被映射到同一位置，因此单词的顺序就不再起作用了；2、为了对单词进行分类，模型会同时使用上下文的一部分单词进行计算；3、模型的作用是根据上下文来推测中间可能出现的单词；4、与传统词袋模型的不同在于其对于上下文的表示是连续。<br>模型复杂度：\(Q=N\times D+D\times log_2(V)\)</p>
</li>
<li><p><strong>连续Skip-gram模型(Continuous Skip-gram Model)</strong><br>模型特点：1、模型的结构类似上述CBOW模型，可以说是“镜像对称”的；2、Skip-gram模型的作用也和CBOW模型相反，其是根据中间的单词，来推测单词的上下文信息；3、考虑到大部分距离较远的单词之间的关联性都较小，因此作者减少了远距离单词的训练样本数量，以降低其权重。<br>模型复杂度：\(Q=C\times (D+D\times log_2(V))\)，C指单词最大化距离，使用模型时会随机选取\([1,C]\)中的一个值作为取词窗口的长度。  </p>
</li>
</ul>
<h3 id="结果分析-Results"><a href="#结果分析-Results" class="headerlink" title="结果分析(Results)"></a>结果分析(Results)</h3><ul>
<li><p><strong>核心思想</strong><br>考虑明显具有相同相似关系的两对单词，如”biggest”、”big”和”smallest”、”small”，计算\(X=D(biggest)-D(big)+D(small)\)，若词向量模型训练成功，应有\(X\)与\(D(smallest)\)是向量空间中距离最近的点。</p>
</li>
<li><p><strong>实验设计</strong><br>作者从句法和语义的角度列出若干相似的类别，分别在每个类别中加入若干单词对，将这些单词对随机两两组合，基于上述公式，计算是否是距离最近的点，若是，即为一次正确的实验结果，最终统计总体正确率来评价模型的优劣。</p>
</li>
<li><p><strong>提升模型精度</strong><br>作者研究了训练数据量和词向量维度两个因素对CBOW模型精度的影响，发现如果只单纯提升某一个因素，对模型精度的影响都是有限的，因此需要同时提升两个因素才能取得比较好的效果。</p>
</li>
<li><p><strong>各模型的对比</strong><br>对比1：在相同训练量和向量维度的条件下，作者对比了RNNLM、NNLM、CBOW和Skip-gram四种模型的精确度，由Table 3可知，CBOW和Skip-gram两种模型的表现明显好于RNNLM、NNLM模型。<br>对比2：作者研究了增大训练量和增大向量维度对CBOW和Skip-gram两种模型的影响，发现只将训练量增大两倍和只将向量维度增大两倍，取得的效果是差不多的，增加的训练时间也比较接近；同时可以发现，两倍数据训练一趟比单倍数据训练三趟的效果要好。<br>对比3：作者将文中的模型与已公布的其他向量模型做了对比，综合表现最好的依旧是Skip-gram。<br>对比4：作者使用了之前提到的分布式计算框架，将向量维度提升到了1000，使用谷歌新闻数据集进行训练，在这种情况下Skip-gram模型的精度依旧是最高的，而且达到了65.6%。<br>对比5：介绍了微软的一个挑战赛，给定挖去一个词的句子和五个候选词，求哪个词和句子最匹配。作者将句子填词问题反向转化为了由词去预测句子的问题，预测出原句的概率最大的那个词，就是所求的词。作者利用Skip-gram+RNNLMs的组合模型，取得了所有模型中最好的效果。</p>
</li>
</ul>
<h2 id="个人感悟"><a href="#个人感悟" class="headerlink" title="个人感悟"></a>个人感悟</h2><h3 id="关于连续词向量模型"><a href="#关于连续词向量模型" class="headerlink" title="关于连续词向量模型"></a>关于连续词向量模型</h3><ul>
<li>本文的工作主要围绕着连续词向量模型，因为传统的词向量表示是将单词视作一个个离散的符号，这样做的缺点是无法提供足够的信息来体现词语之间的关联，比如虽然Italy和Rome两个单词看起来一点也不像，但其实两者之间是有联系的(Rome是Italy的首都)，而如何研究出这种潜在的联系，便是本文的主要讨论内容。  </li>
<li>作者的思路是通过词向量来表征这种潜在的联系，首先使用了两种神经网络来生成词向量，但因为隐含层的计算开销过大，使得模型无法训练大量数据集，也无法提升词向量的维度；因此作者从简化模型的角度，提出CBOW和Skip-gram两种模型，并且使用了分布式训练框架，这使得模型能够针对大量数据集进行训练，从而获得更好的训练效果。  </li>
<li>训练出向量模型后，作者为了检测向量模型的效果，基于“相似度高的向量距离近”的思想，提出了一种检测算法，对向量进行代数运算并计算之间的距离，如果相似度高的向量计算出的距离确实小，就说明模型是合理的，作者即通过这个思路分析了各个模型的优劣。</li>
</ul>
<h3 id="关于词向量的线性运算"><a href="#关于词向量的线性运算" class="headerlink" title="关于词向量的线性运算"></a>关于词向量的线性运算</h3><ul>
<li>文中所用的检测词向量精度的方法是基于词向量的线性运算，即“意大利-罗马+巴黎=法国”这样的计算规则，通过这个算式，我们可以很明显地感受到词之间所具有的某种关联，利用这种关联就可以大大扩展词向量的应用范围。  </li>
<li>但具体为什么会有这种性质呢？我认为可能的原因：词向量的假设是基于上下文的分布来推导词义，而“意大利-罗马+巴黎=法国”可以转换为“意大利+巴黎=法国+罗马”，而只有意大利和巴黎共同的上下文与法国和罗马共同的上下文是相似的，才会有这样的相等关系。而仔细考虑一下，这两对词的上下文确实是有可能比较像的，比如一篇介绍欧洲国家的文章，这些词所在的语境肯定是很相近的，这也就是我们把这些词作为相似词的原因。</li>
</ul>
<h3 id="关于训练数据量"><a href="#关于训练数据量" class="headerlink" title="关于训练数据量"></a>关于训练数据量</h3><ul>
<li>本文还有一个比较重要的思想是：简化模型，提升学习效率，利用更大的训练量来求得更好的训练效果。作者一开始就是因为神经网络模型的计算量太大，因此舍弃了隐含层，简化模型，从而提出了CBOW和Skip-gram这两个模型。</li>
<li>这样做虽然可能会损失一部分隐含层所提升的精度，但计算开销大大降低了，结合分布式学习模型，可以极大地提升训练量和向量维度的上限，从而弥补因简化模型而损失的精度。实验结果证明了这样的方案是可行的，这也为我们今后如何改善模型精度提供了一个思路。</li>
</ul>
<h2 id="改进方案"><a href="#改进方案" class="headerlink" title="改进方案"></a>改进方案</h2><h3 id="关于CBOW模型的语义问题"><a href="#关于CBOW模型的语义问题" class="headerlink" title="关于CBOW模型的语义问题"></a>关于CBOW模型的语义问题</h3><ul>
<li>在CBOW模型中，所有的单词最终都会映射到同一位置，模型是不考虑单词顺序的。我认为这样做主要是因为CBOW模型的作用是从上下文中推测相关的单词，即模型关注的是文章中是否出现单词，并不关注单词出现的顺序，因此舍弃顺序也是合理的。  </li>
<li>但这样做也会有一定问题，即一些语句虽然单词组成一样，但语义却有明显不同。如“李丽是谁的姐姐”和“李丽的姐姐是谁”，这两句话的词袋模型是完全一致的，但如果不考虑语法结构，我们很难得出正确的结果。因此在这种情况下，我们可以将词袋模型和句法分析相结合，来求出句子的真正含义。</li>
</ul>
<h3 id="CBOW模型和Skip-gram模型与神经网络相结合"><a href="#CBOW模型和Skip-gram模型与神经网络相结合" class="headerlink" title="CBOW模型和Skip-gram模型与神经网络相结合"></a>CBOW模型和Skip-gram模型与神经网络相结合</h3><ul>
<li>文章最后提到神经网络词向量和其他技术的结合可能会有很好的效果，作者在微软的挑战赛中也通过Skip-gram+RNNLMs取得了很高的预测精度，说明这个结合思路是可行的。  </li>
<li>因为首先CBOW和Skip-gram都未考虑单词的顺序，会带来一定的语义问题；同时RNN模型本身具有自连接性质，这可以使其对前后的单词的关联性有更好的“记忆”，因此两者结合可能会有互补的效果。关于实现，可以将两者的词向量模型以一定权重进行组合，得出新的词向量，以探究其在相关问题中的表现效果。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://yoursite.com/2018/09/13/Paper Notes/Efficient Estimation of Word Representations in Vector Space/" data-id="cjqqhv6ix002c3hqxqtqso4sf" class="article-share-link" data-share="baidu" data-title="Notes of Efficient Estimation of Word Representations in Vector Space读书笔记">Share</a>
      

      
        <a href="http://yoursite.com/2018/09/13/Paper Notes/Efficient Estimation of Word Representations in Vector Space/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Paper-Notes/">Paper Notes</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Paper Notes/Personal Recommendation Using Deep Recurrent Neural Networks in NetEase" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/09/13/Paper Notes/Personal Recommendation Using Deep Recurrent Neural Networks in NetEase/" class="article-date">
  <time datetime="2018-09-13T07:49:59.000Z" itemprop="datePublished">2018-09-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/09/13/Paper Notes/Personal Recommendation Using Deep Recurrent Neural Networks in NetEase/">Personal Recommendation Using Deep Recurrent Neural Networks in NetEase读书笔记</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p><a href="http://cfm.uestc.edu.cn/~zhangdongxiang/papers/ICDE16_industry_231.pdf" target="_blank" rel="noopener">论文地址点这里</a></p>
<p><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script>  </p>
<h2 id="论文内容概述"><a href="#论文内容概述" class="headerlink" title="论文内容概述"></a>论文内容概述</h2><p>本文结合RNN和FNN两种神经网络提出了一种新的个性推荐方法，希望解决传统的CF方法无法进行实时推荐的问题，最终在网易的考拉电商网站上取得了良好的效果。</p>
<h3 id="简介-INTRODUCTION"><a href="#简介-INTRODUCTION" class="headerlink" title="简介(INTRODUCTION)"></a>简介(INTRODUCTION)</h3><ul>
<li><p><strong>传统CF模型存在的问题</strong><br>因为是基于用户购物习惯的推荐，使用的是历史信息，未能利用用户当前的浏览历史，因此无法进行实时推荐。</p>
</li>
<li><p><strong>如何进行实时推荐</strong><br>首先要考虑访问电商网站的用户属性，包括基本属性(浏览器、IP地址、个人基本信息、购买历史等)和动态属性(用户所浏览页面的信息)，而基于后者，我们就可以猜测用户在本次访问中究竟想要购买什么，从而完成实时推荐。</p>
</li>
<li><p>之后作者对系统主要面临的挑战、所构建DRNN的特点和其他一些技术作了介绍，即完成了本节内容。</p>
</li>
</ul>
<h3 id="推荐模块概述-OVERVIEW-OF-RECOMMENDATION-MODULE"><a href="#推荐模块概述-OVERVIEW-OF-RECOMMENDATION-MODULE" class="headerlink" title="推荐模块概述(OVERVIEW OF RECOMMENDATION MODULE)"></a>推荐模块概述(OVERVIEW OF RECOMMENDATION MODULE)</h3><ul>
<li><p><strong>系统流程</strong><br>服务器首先接收用户请求并聚合为一个会话信息，之后将其输入到推荐系统中，经过RNN和FNN计算后输出推荐结果，并在页面中显示。</p>
</li>
<li><p><strong>数据格式</strong><br>数据收集：假设用户\(u_i\)访问网站，会生成日志文档\(D_j^l\)和会话文档\(D_i^s\)两种文档，两者关系为\(D_i^s=\{D_0^l,D_1^l,\cdots,D_{k-1}^l\}\)，即一个会话文档对应多个日志文档。<br>数据简化：又因为每个日志文档都可简化为一个URL地址\(p_j\)，因此可得\(D_i^s=\{p_0,p_1,\cdots,p_{n-1}\}\)。<br>最终的输入数据：访问网站的每个用户\(u_i\)都会对应一个\(D_i^s\)，即构成了神经网络的输入数据。</p>
</li>
</ul>
<h3 id="DRNN具体介绍-DEEP-RECURRENT-NEURAL-NETWORK"><a href="#DRNN具体介绍-DEEP-RECURRENT-NEURAL-NETWORK" class="headerlink" title="DRNN具体介绍(DEEP RECURRENT NEURAL NETWORK)"></a>DRNN具体介绍(DEEP RECURRENT NEURAL NETWORK)</h3><ul>
<li><p><strong>RNN的特点</strong><br>RNN即循环神经网络，相比其他神经网络最大的特点是：其考虑了前后两个状态之间的关联，可以更好地处理序列信息。在本文的场景中，一个session可抽象为一系列的网页序列，因此利用RNN来进行推荐直观上是非常合适的。</p>
</li>
<li><p><strong>基本RNN模型</strong><br>在单个隐藏层的RNN中，隐藏层节点除了的输入和输出外，还会有一个自连接环，可以根据时间来不断地更新它的值。<br>更新函数：\(a(i)=f(Ux(i)+Wa(i-1))\)，\(a(i)\)表示在状态\(i\)下的节点值，\(x(i)\)表示输入值，\(U,W\)为相应的转移矩阵，\(f(x)\)为激活函数。<br>公式理解：隐藏层节点每次更新除了会考虑输入值外，还会考虑该节点在前一状态下的值，因此RNN的结果可以反映时间序列的相应信息。</p>
</li>
<li><p><strong>有限状态的DRNN</strong><br>当RNN具有多个隐藏层时，即构成了DRNN。考虑DRNN中第\(i\)层的某个状态\(t\)，其不仅会连接同层的状态\(t+1\)，还会连接到第\(i+1\)层的状态\(t\)，即构成了新的更新函数：<br>$$ f(x)=\left\{<br>\begin{align}<br>&amp;f(W_ia_i(t-1)+Z_i(a_{i-1}(t)+b_i(t)))&amp;,&amp; i&gt;1 \\<br>&amp;f(W_ia_i(t-1)+Z_i(V_t+\theta (p_t)))&amp;,&amp; i=1<br>\end{align}<br>\right.<br>$$</p>
</li>
<li><p><strong>引入历史状态节点的DRNN</strong><br>问题背景：受限于内存，我们不可能保存用户所有产生的状态；但如果使用\(n\)状态的滑动窗口，则只能选择最新的\(n\)个数据训练模型，会降低预测精度。因此作者引入了历史状态节点(history state)的概念。<br>当用户访问的页面数\(x\)超过一定数量\(n\)时，我们将前\(x-n\)个状态组合起来作为历史状态节点，有<br>$$\bar{V}=\sum_{i=0}^{x-n}\varepsilon_iV_i$$<br>$$\varepsilon_i=\frac {\theta(p_i)} {\sum_{j=i}^{x-n}\theta(p_j)} $$<br>公式理解：根据用户在页面的停留时间对前\(x-n\)个页面作加权平均，近似表征用户的历史信息，是一个既在一定程度上保证了模型精度，计算开销又不至于太大的折中方案。</p>
</li>
<li><p><strong>与协同过滤算法的结合</strong><br>问题背景：虽然协同过滤算法无法提供实时的推荐，但如果用户遵循以往购买习惯，其推荐效果还是很好的。因此作者引入了FNN模型来模拟CF算法，作为RNN的补充。<br>另外使用FNN还有两点好处：</p>
<ul>
<li>FNN和RNN共享相同的输出层，因此可以将二者的输出融合起来作为最终结果，来表征用户购买某件商品的概率。</li>
<li>可以使用随机最速下降法(SGD)来训练RNN和FNN结合的权重，而不用人为地决定哪个网络更为重要，减轻了模型调参的工作量。</li>
</ul>
</li>
<li><p><strong>如何生成训练数据</strong><br>用户从进入网站开始到最终购买商品，会经历一定数量的页面，个性推荐的本质目标就是减少这之间页面的数量。对于一次购买行为\(I\)，其对应的页面路径为\(p_0,\cdots,p_{n-1}\rightarrow I\)。若对其进行优化，不一定非要优化成\(p_0\rightarrow I\)这样(当然这是最优情况)，只要能减少用户的页面访问数量，都可以算作优化。因此我们的训练数据还可以是\(p_0,p_1\rightarrow I\)、 \(p_0,p_1,p_2\rightarrow I\)等，这样一次购买行为就可以产生\(n\)组训练数据，大大增加了我们的训练量。</p>
</li>
<li><p><strong>模型的实现</strong><br>作者使用了Caffe框架来实现模型，整个网络包含三层隐含层，且同一层次的神经元共享相同的权重和偏置矩阵。此外，模型的RNN部分包含4个状态的输入，FNN包含1个状态的输入。</p>
</li>
</ul>
<h3 id="模型调优-MODEL-OPTIMIZATIONS"><a href="#模型调优-MODEL-OPTIMIZATIONS" class="headerlink" title="模型调优(MODEL OPTIMIZATIONS)"></a>模型调优(MODEL OPTIMIZATIONS)</h3><p>这一节作者介绍了其为了改进模型性能所做的工作，并提出了一种自动调优框架，使得模型具有了更高的精度和更快的学习速度。</p>
<ul>
<li><p><strong>自动代码生成器</strong><br>问题背景：模型中包括了很多参数，调整这些参数需要更改甚至重写Caffe脚本，非常繁琐。因此作者构建了一个代码生成器，其主要任务是接收参数值，输出对应的Caffe脚本。<br>主要思想：首先将参数分为<strong>基本参数</strong>(损失函数、学习速率等)和<strong>网络结构参数</strong>(每层的神经元数)，调整基本参数只需更改相应的值，而调整网络结构参数则需要改写Caffe脚本，因此我们只需重点关注网络结构参数即可。<br>三种网络结构参数：文中提出了长、宽、高三种网络结构参数，分别对应隐含层数量、状态数以及隐含层与状态层的连接数，并编写了相应的代码生成算法，具体可见文中的Algorithm 1。</p>
</li>
<li><p><strong>模型调优</strong><br>为了求得表现更好的模型参数，作者采用了遗传算法这种启发式算法进行模型调优。<br>染色体结构：\(C=(w,l,h,a_1,a_2,\cdots,a_L,\cdots)\)，直观的理解就是将所有参数结合在了一起。<br>适应度函数：\(fit=accuracy+\frac {1} {1+loss}\)，\(accuracy\)为模型预测精度，\(loss\)为损失率。<br>注：虽然遗传算法最终求得的是局部最优解，但因为参数调优本就是一个非常复杂、难以建模的过程，所以作者认为这样的解已经足够好了。</p>
</li>
</ul>
<h3 id="模型实验-EXPERIMENTS"><a href="#模型实验-EXPERIMENTS" class="headerlink" title="模型实验(EXPERIMENTS)"></a>模型实验(EXPERIMENTS)</h3><p>这一节作者对模型进行了全面的测试和分析，并分别研究几个重要因素对模型的影响。</p>
<ul>
<li><p><strong>评价指标</strong><br>作者采用了预测正确率作为模型主要的评价指标，公式为\(accuracy=\frac {f(S)} {|S|}\)，\(S\)代表训练样本总数，\(f(S)\)代表正确预测的样本数。</p>
</li>
<li><p><strong>batch size的影响</strong><br>在使用默认参数训练模型时，增大batch size可以提高模型精度，但对于内存的消耗也更大。而一个有趣的现象是，使用调优框架后再训练模型，batch size对精度的影响就不再显著了，可见调优框架确实使得模型的表现更为优异了。</p>
</li>
<li><p><strong>FNN的影响</strong><br>由Fig.11、Fig.12可得，FNN的使用显著提升了模型精度，尤其是同时使用调优框架和FNN的模型，精度提升了约10%，而且这还是在模型只推荐1个物品的情况下(即购买概率最大的那个物品)，若模型返回10个物品，模型精度可以达到50%以上。<br>同时，使用FNN并不会影响模型的收敛速率，即模型精度的提升并不会增加计算开销，这也是很重要的一点。</p>
</li>
<li><p><strong>历史状态节点的影响</strong><br>在使用默认参数训练模型时，使用历史状节点态可提高模型10%的精度；而使用了调优框架后，只能提升2%的精度，即调优过程降低了不同网络结构对模型的影响，这也侧面表明了作者所提出调优框架的优异性能。</p>
</li>
<li><p><strong>模型最终效果</strong><br>在DRNN和FNN结合的情况下，模型最终的预测精度达到了33.13%，页面路径压缩到了原先的72.41%，相当于用户从点进网站到最终购买商品，少浏览了30%的页面，效果还是显而易见的，毕竟用户每多浏览一个自己不感兴趣的页面，其离开网站的概率就会越大，这也就是推荐系统的作用所在，即帮助用户更快地进行选择。</p>
</li>
</ul>
<h2 id="个人感悟"><a href="#个人感悟" class="headerlink" title="个人感悟"></a>个人感悟</h2><h3 id="关于推荐系统"><a href="#关于推荐系统" class="headerlink" title="关于推荐系统"></a>关于推荐系统</h3><ul>
<li>本文提出了基于RNN和FNN的推荐系统，最主要的原因是传统的CF算法基于用户间购物历史的相似度来做推荐，考虑的更多是用户的购物习惯，这就使其无法把握用户突发的、低频的购买需求。</li>
<li>比如我平常很爱买衣服，网购的大部分物品都是衣服，但有一天突然想吃零食，就会在网站上浏览零食的相关页面，这时系统给我推荐的若还是衣服，就会影响用户体验，也不利于提高网站的销量，因此实时推荐是很有必要的。</li>
<li>但同时，在生活大部分情况中，一个人还是会遵循其所形成的习惯去购物，这时基于购物习惯的推荐便能表现出很好的效果，因此CF算法也是很有必要的。</li>
<li>综上，论文中使用RNN和FNN两种算法共同完成推荐，这种思路是合理的，也是符合我们的直观认知的。</li>
</ul>
<h3 id="关于RNN算法"><a href="#关于RNN算法" class="headerlink" title="关于RNN算法"></a>关于RNN算法</h3><ul>
<li>RNN与其他网络最大的不同在于其隐含层节点的自连接性，在其更新函数中，不仅包括正常的输入值，还包括上一时刻中节点自身的值，这就像使得节点具有了“记忆”，这个记忆表征了时间序列中前后节点的关联，因此RNN适合处理时间序列或状态间具有一定联系的情况。</li>
<li>对应到本文，用户在网站上购物必然会浏览一系列页面，而这些页面是否是有关联的呢？我认为是有的。比如我要买一个钱包，在第一个页面中没有浏览到我喜欢的款式，那我下一个访问的页面也会是关于钱包的，甚至我之后的浏览可能都会围绕钱包来展开，因此我们就可从用户初始的浏览内容来推测其实时的购买兴趣，这也就是作者应用RNN进行实时推荐的原因。</li>
</ul>
<h3 id="关于历史状态节点-history-state"><a href="#关于历史状态节点-history-state" class="headerlink" title="关于历史状态节点(history state)"></a>关于历史状态节点(history state)</h3><ul>
<li>本文的一个创新在于引入了历史状态节点，来解决状态数过多、计算开销过大的问题。根据用户在页面上停留的时间，将多出的历史状态作加权平均，构成一个新的节点，这样既可以保留历史信息，又不会造成计算开销的大量增加，是一种比较折衷的方案。</li>
<li>这也提示我们，在数据量过大的情况下，与其直接将一部分数据丢弃掉，不如将这部分数据做整合，采用加权平均或其他的提取信息的方法，构成新的节点来参与运算，在模型精度和计算开销之间取得平衡。</li>
</ul>
<h3 id="关于训练数据的生成"><a href="#关于训练数据的生成" class="headerlink" title="关于训练数据的生成"></a>关于训练数据的生成</h3><ul>
<li>本文针对用户一次的购买行为，将各种可能的路径优化方案都作为了训练数据送入模型进行训练，这样可以增加我们的训练量。因为对于有监督学习来说，带标签的数据是有限的，如何充分利用有限的标签数据去训练模型需要我们去研究，本文给了我们一种可行的思路。</li>
</ul>
<h3 id="关于模型调优"><a href="#关于模型调优" class="headerlink" title="关于模型调优"></a>关于模型调优</h3><ul>
<li>本文提出了一种模型调优框架，大致可分为“生成参数”和“生成代码”两部分。</li>
<li>首先将模型的相关参数整合成染色体，再利用遗传算法的杂交、变异等操作，生成下一代染色体，之后将参数传入代码生成器中生成对应的Caffe脚本，训练模型后，将结果回带到遗传算法的适应度函数中进行评估和自然选择，一轮轮地迭代，最终即可得出最优的参数组合。</li>
<li>因为建模的复杂性和组合爆炸等问题，模型调优一直是机器学习的一个难点，在这种情况下，利用一些启发式算法进行智能调优不失为一个好的方法，这也是文章给我们的一个启示。</li>
</ul>
<h2 id="改进方案"><a href="#改进方案" class="headerlink" title="改进方案"></a>改进方案</h2><h3 id="关于模型的更新问题"><a href="#关于模型的更新问题" class="headerlink" title="关于模型的更新问题"></a>关于模型的更新问题</h3><ul>
<li>在文章中提到，顾客每完成一次购物，就相当于得到了一个”ground truth”，可以用来训练、调整模型，这样模型随着网站的运行就会不断优化和改进。</li>
<li>如果顾客这次购物是在遵循自己以往的购物习惯，那么将结果用来继续训练模型是没有问题的，因为这个行为在今后还会多次发生，这样可以使得模型更了解顾客的购买习惯，从而更好地完成推荐。</li>
<li>但如果这是一次”unexpected”的购物行为，就像平常都喜欢买衣服的我只是突然想吃点零食，如果模型把这个突发的低频需求当作了用户的购买习惯，在今后也多次向用户进行推荐，可能会造成用户的厌烦。</li>
<li>因此我认为在模型的持续更新中，应该考虑到用户购买行为的属性，即对利用了RNN方式完成的推荐，要降低其对模型的后续影响。</li>
<li>方案实现：可以通过减少训练样本数量的方式，对于通过RNN推荐完成的购买，不要将<br>$$<br>  p_0 \rightarrow I \\<br>  p_0,p_1 \rightarrow I \\<br>  \cdots \\<br>  p_0,\cdots,p_{n-1} \rightarrow I<br>$$<br>所有的路径样本都送入训练，可以只送入后半段或后1/4的样本，让推荐发生的条件更为“苛刻”一些，从而使得模型不会在用户一进入网站就推荐一些低频、不经常需求的产品。</li>
</ul>
<h3 id="将社群属性加入到模型中"><a href="#将社群属性加入到模型中" class="headerlink" title="将社群属性加入到模型中"></a>将社群属性加入到模型中</h3><ul>
<li>论文在”Related Work”中提到CF推荐和基于内容的推荐相结合，会在社交网络上表现得更好，因此我认为可以将这种思路应用到本文，即在推荐系统中加入一定的社群属性。</li>
<li>问题背景：在实际生活中，我们会更倾向于接受来自朋友的推荐，而非来自商家的推荐。尤其是本文的电商网站——网易考拉，是一个主打海淘的平台，用户对一些国外的品牌可能了解并不多，这时如果有来自自己社交圈的推荐，无疑会增加购买的概率。</li>
<li>方案实现：可以考虑改进文中的FNN部分的方法，即首先通过用户填写的基本信息或其他网易系应用中的用户资料(如网易音乐、游戏等)，对用户进行社群判别和分类，为每个用户构建一个“熟人圈”。在进行FNN推荐时，模型不仅推荐历史相似的用户购买的产品，也推荐来自熟人圈购买的产品，以将社群属性加入到推荐模型中。</li>
</ul>
<h3 id="关于推荐理由"><a href="#关于推荐理由" class="headerlink" title="关于推荐理由"></a>关于推荐理由</h3><ul>
<li>问题背景：目前用户对于推荐系统的态度，不仅是想“知其然”，也想“知其所以然”，即除了推荐的物品本身，用户也会想知道系统为什么会给自己推荐这个物品，因此如果能给推荐物品附上推荐理由，无疑会提升网站的用户体验。</li>
<li>方案实现：因为本文是将两种神经网络的输出层共享，以类似加权平均的方式进行融合(见文章第三节D部分)，来计算出用户购买某件物品的概率的，因此可以将最终结果的各个部分分离出来，从大到小排序，通过分析值最大的一项或几项来构造我们的推荐理由。</li>
<li>具体形式：推荐理由的形式可以为“根据你以前购买过的xxx牛仔裤，我们猜你还喜欢这个”、“根据你刚刚浏览过的xxx钱包，我们猜你会喜欢这个”等等，以一种猜测、活泼的口吻对用户进行提示，以增加用户的购买欲望。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://yoursite.com/2018/09/13/Paper Notes/Personal Recommendation Using Deep Recurrent Neural Networks in NetEase/" data-id="cjqqhv6iz002h3hqxzq7uykvs" class="article-share-link" data-share="baidu" data-title="Personal Recommendation Using Deep Recurrent Neural Networks in NetEase读书笔记">Share</a>
      

      
        <a href="http://yoursite.com/2018/09/13/Paper Notes/Personal Recommendation Using Deep Recurrent Neural Networks in NetEase/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Paper-Notes/">Paper Notes</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Paper Notes/On Availability For Blockchain based Systems" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/09/13/Paper Notes/On Availability For Blockchain based Systems/" class="article-date">
  <time datetime="2018-09-13T07:49:59.000Z" itemprop="datePublished">2018-09-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/09/13/Paper Notes/On Availability For Blockchain based Systems/">On Availability For Blockchain based Systems读书笔记</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p><a href="https://research.csiro.au/data61/wp-content/uploads/sites/85/2016/08/OnAvailabilityForBlockchain-BasedSystems-SRDS2017-authors-copy.pdf" target="_blank" rel="noopener">论文地址点这里</a></p>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script>  

<h2 id="论文内容概述"><a href="#论文内容概述" class="headerlink" title="论文内容概述"></a>论文内容概述</h2><p>这是一篇偏重实验的论文，通过研究影响交易提交时间的因素，探讨了目前区块链的可靠性问题，并提出了一种交易终止策略来解决这个问题。</p>
<h3 id="简介-INTRODUCTION"><a href="#简介-INTRODUCTION" class="headerlink" title="简介(INTRODUCTION)"></a>简介(INTRODUCTION)</h3><ul>
<li><p><strong>区块链的可靠性问题</strong><br>可靠性保证不透明：从应用的角度上来说，并不清楚区块链技术是如何保证系统的可靠性的，而且多数区块链系统只能以一定概率保证交易信息的不变性。<br>提交时间不稳定：客户端不可预知交易成功提交所需的时间。从Fig.1中可看出，61.5%的交易在3分钟内就提交了，但13.8%的交易4.5分钟还没提交(延迟了50%)，提交时间的差异可能系统可用性的下降。</p>
</li>
<li><p><strong>论文主要研究内容</strong>  </p>
<ul>
<li>导致交易无法成功提交的因素</li>
<li>以太坊中的transaction inclusion机制</li>
<li>以太坊中gas price和gas limit两个参数对提交时间的影响</li>
<li>以太坊中block gas limit的影响</li>
<li>交易终止机制</li>
</ul>
</li>
</ul>
<h3 id="背景知识-BACKGROUND"><a href="#背景知识-BACKGROUND" class="headerlink" title="背景知识(BACKGROUND)"></a>背景知识(BACKGROUND)</h3><ul>
<li><p><strong>区块链基本知识</strong><br>区块链可认为是一个分布式的公共账本，记载了交易信息和资产信息。区块链中的每个用户本地都存储着一个账本的副本，并运行着一个客户端，负责和整个网络同步更新账本信息。</p>
</li>
<li><p><strong>区块链中的链指什么</strong><br>区块链系统的交易信息存储在区块(block)中，从第二个区块开始，每个区块都有前一区块的哈希值，即相当于把各个区块链起来了，最终各个区块按时间顺序链接起来呈现一套完整的数据(区块链大账本)。 </p>
</li>
<li><p><strong>区块链加密技术</strong><br>区块链采用了非对称加密技术，和传统对称加密最大不同是，加密和解密不再是同一把钥匙，而是分为公钥和私钥。只有特定的用户具有私钥，可以对交易信息进行加密和数字签名；而所有的用户都具备公钥，可以对交易信息解密，并通过数字签名验证交易签署者的身份。 </p>
</li>
<li><p><strong>共识算法</strong><br><strong>问题背景：</strong>因为区块链是分布式存储的，所有节点都需要对区块链中块的内容和次序达成“共识”，为了解决这个问题就提出了共识算法。<br><strong>工作量证明算法(POW)：</strong>这是目前最广泛使用的共识算法，其基本原理是：一个节点将交易信息打包到区块中，并添加一个随机数，再做哈希和运算，之后将块发布到网络中。其他节点接收到块后，根据块中包含的参数进行大量的计算(俗称挖矿)，计算出的结果如果小于哈希和，即完成了这部分工作，得出的结果值就相当于对其工作量的“证明”。拥有“证明”后节点不仅会获得一定奖励，也拥有了创建区块的权利，新创建的区块会加到主链末端，其他节点可在其后面继续添加新区块，其后每添加一个区块，就相当于做了一次确认，当其后的区块达到了一定数量后(比特币是6个，以太坊是12个)，就相当于和网络中其他节点达成“共识”了，这个区块即正式被记录在案了。<br><strong>核心思想：</strong>工作量证明机制的核心思想是工作方要得出结果具有一定难度，但验证方检查结果却非常容易，因此很容易验证工作方是不是做了相应工作。既保证了工作完成后奖励的价值(需要付出大量的计算代价)，激励节点去积极打包交易信息，同时验证的简单性也保证了共识算法的效率和可靠。</p>
</li>
<li><p><strong>分叉问题(fork)</strong><br><strong>基本概念：</strong>考虑一种小概率情况，两个节点恰好同时完成了计算，创建了区块，并向全网进行了广播，这种情况就叫做分叉。形象的理解就是区块链主链的末端被连了两个块，像是分叉了一样。<br><strong>分叉的影响：</strong>正常情况下，网络中所有的节点达成共识，所生成的区块链就是一条主链，内容、次序都是确定的；而在分叉的情况下，区块链会向两个不同的方向延申(因为每一个区块都是依赖于上一个区块产生的)，主链的内容、次序就不确定了，各节点即无法正常地查询交易和资产信息。<br><strong>解决办法：</strong>所有节点都从当前最长的链开始工作。因为有统计结论，链中的区块数越多，越不容易发生分叉现象。</p>
</li>
</ul>
<h3 id="比特币交易的提交-COMMIT-OF-BITCOIN-TRANSACTIONS"><a href="#比特币交易的提交-COMMIT-OF-BITCOIN-TRANSACTIONS" class="headerlink" title="比特币交易的提交(COMMIT OF BITCOIN TRANSACTIONS)"></a>比特币交易的提交(COMMIT OF BITCOIN TRANSACTIONS)</h3><p>在本节中，作者讨论了影响比特币提交时间的因素，并进行了实验验证。</p>
<ul>
<li><p><strong>影响交易提交时间的因素</strong><br><strong>交易手续费(transaction fee)：</strong>手续费的多少会影响节点打包交易信息的积极性，从而影响交易提交的时间。<br><strong>交易是否按顺序到达：</strong>如果一个交易比其所引用的父交易先到达，则被称为orphan。只有父交易提交后，子交易才能提交，否则子交易就会一直在mempool中等待。<br><strong>锁定时间(locktimes)：</strong>这是一个用户可设置的参数，可使得某个交易在特定顺序的区块产生之前一直处于不可用状态。  </p>
</li>
<li><p><strong>实验过程</strong><br><strong>实验任务：</strong>观测交易的产生并记录其提交所花费的时间。为了探究不同网络环境下的情况，作者在2016年11月和2017年4月分别进行了两次实验（第二次实验的网络负载较大）。<br><strong>观测窗口：</strong>为了充分收集包含在区块链中的交易信息，作者定义了观测窗口：从实验开始前的第一个区块到实验结束24h后的一个区块，这之间的交易即处于观测窗口中，都会被记录下来。</p>
</li>
<li><p><strong>实验结果</strong><br>作者主要研究的交易有两类：<strong>Straight-accepts</strong>和<strong>Oranphans</strong>。<br>实验结果1（针对到达顺序因素）：由Fig.2可知两次实验中，Oranphans的提交时间都比Straight-accepts的时间要长，且在第二次实验中尤为明显，可见到达的顺序对提交时间有着显著的影响，按顺序到达的交易比无序到达的交易时间要短很多。<br>实验结果2（针对交易手续费因素）：由Fig.3可知，交易手续费和提交时间之间并没有显著关联。<br>实验结果3（针对locktimes因素）：通过作者分析可知，绝大多数交易并未使用locktimes这个参数，且Oranphans和Straight-accepts这两类交易结束锁定的时间差异也很大，最终作者认为locktimes并非是Oranphans延迟的主要因素。  </p>
</li>
</ul>
<h3 id="以太坊交易的提交-COMMIT-OF-ETHEREUM-TRANSACTIONS"><a href="#以太坊交易的提交-COMMIT-OF-ETHEREUM-TRANSACTIONS" class="headerlink" title="以太坊交易的提交(COMMIT OF ETHEREUM TRANSACTIONS)"></a>以太坊交易的提交(COMMIT OF ETHEREUM TRANSACTIONS)</h3><p>在本节中，作者介绍了以太坊不能保证提交的原理，并实验研究了gas price、gas limit、network三个因素对提交时间的影响。</p>
<ul>
<li><p><strong>以太坊中交易事务的生命周期</strong><br>1、交易声明：交易发生并声明。<br>2、交易打包进区块：发布节点将交易信息打包到区块中。<br>3、区块链接到主链：节点完成打包计算，将区块链接到主链。<br>4、交易正式提交：其后链接了一定数量的区块后，即正式提交。<br>注：因为分叉现象或其他原因，步骤2并不能保证交易最终一定被提交，这就产生系统的可靠性问题。</p>
</li>
<li><p><strong>实验过程</strong><br>实验任务：研究交易从打包到最终提交所花费的时间以及分支合并后会丢失多少已打包的交易。<br>创建监听节点：作者改写了一个客户端节点作为监听节点，以检测交易声明和区块声明。<br>记录时间：监听节点会记录交易声明的时间和区块到达的时间，以计算交易提交的延迟时间。<br>计算方法：根据交易声明的时间和包含此交易的第一个区块到达的时间，我们计算这之间的时间差，结果就是Fig.5中的<br><code>1st inclusion</code>；当第一个块变为叔块时，我们可根据包含此交易的第二个区块到达的时间，计算出<code>2nd inclusion</code>；同理，<br><code>3rd inclusion</code>也是这么计算的；此外，我们根据最后一个区块的到达时间，可以计算出<code>12 confirmations</code>和<code>36 confirmations</code>。将上述计算出的结果绘成图表，就是文中的Fig.5。</p>
</li>
<li><p><strong>实验结果</strong><br>实验结果1（各类提交时间的对比）：从Fig.5中可知，相比1st/2nd/3rd inclusion，12/36 conclusion的提交时间更长，而36 conclusion尤为明显，这说明交易数量越多，交易提交所花费的时间越长。<br>实验结果2（针对gas price因素）：从Fig.6中可知，总体趋势是gas price越高，延时越短，但gas price高于25Gwei后对延时的影响就非常小了。这也侧面解释为什么大部分交易定价在[20,25)的区间内(性价比最高)。<br>实验结果3（针对maximum gas因素）：虽然有个别交易因maximum gas过高而有明显的延时，但作者依旧认为maximum gas和提交时间之间没有很强的关联性。<br>实验结果4（针对network delays因素）：虽然没有得出明确的结论，但作者分析了in-order和out-of-order交易在提交延迟和数量方面的数据后，认为网络延迟对交易的传播是有负面影响的。</p>
</li>
</ul>
<h3 id="BLOCK-GAS-LIMIT对以太坊的影响-IMPACT-OF-THE-BLOCK-GAS-LIMIT-IN-ETHEREUM"><a href="#BLOCK-GAS-LIMIT对以太坊的影响-IMPACT-OF-THE-BLOCK-GAS-LIMIT-IN-ETHEREUM" class="headerlink" title="BLOCK GAS LIMIT对以太坊的影响(IMPACT OF THE BLOCK GAS LIMIT IN ETHEREUM)"></a>BLOCK GAS LIMIT对以太坊的影响(IMPACT OF THE BLOCK GAS LIMIT IN ETHEREUM)</h3><ul>
<li><p><strong>产生原因</strong><br>出台gas limit per block的原因是希望通过限制每个区块消耗的gas总量，防止DDoS攻击。如果交易所需的气体超过了限制值，交易就无法被包含到块中，这就使得大规模的分布式拒绝服务攻击难以发生。</p>
</li>
<li><p><strong>问题背景</strong><br>在没有限制之前，签署合约花费了150万的气体，因此作者认为限制会对合约相关的交易产生负面影响，但对单纯的资金转移交易应该影响不大，之后作者即针对这个观点进行了分析。</p>
</li>
<li><p><strong>分析结果</strong><br>由Fig.10可知，在50万气体的限制下，有46.21%的合约类交易无法正常进行；在200万气体的限制下，也有18.78%的合约类交易无法产生。因为气体限额的存在，大量的合约类交易无法进行，这也验证了作者之前的观点。</p>
</li>
</ul>
<h3 id="以太坊的交易终止机制-TRANSACTION-ABORT-IN-ETHEREUM"><a href="#以太坊的交易终止机制-TRANSACTION-ABORT-IN-ETHEREUM" class="headerlink" title="以太坊的交易终止机制(TRANSACTION ABORT IN ETHEREUM)"></a>以太坊的交易终止机制(TRANSACTION ABORT IN ETHEREUM)</h3><p>这一节主要对应了作者在摘要中提到的观点：终止机制的缺失会使大量交易处于既未终止也未提交的”pending”状态，严重影响系统的可靠性。为解决这个问题，作者提出了一种交易终止的机制，并进行了实验考察。</p>
<ul>
<li><p><strong>终止原理</strong><br><strong>竞争法：</strong>如果先前的交易\(Tx_i\)在规定的时间内未提交，账户可以重新发布一个具有相同nonce序号的交易\(Tx_i^{\prime}\)，赋予其更高的手续费，并将交易的接收方设为自己。一旦\(Tx_i^{\prime}\)成功提交，\(Tx_i\)就会自动过期了，即终止了其状态。<br><strong>重传法：</strong>账户重新发布一个与\(Tx_i\)内容相同的交易\(Tx_i^{\prime\prime}\)，但设置较高的手续费，这样虽然\(Tx_i^{\prime\prime}\)交易信息和\(Tx_i\)是一样的，但数字签名和哈希值却不一样(因为手续费不一样)，这样其他节点会把其当作一个新的交易。只要交易\(Tx_i\)和\(Tx_i^{\prime\prime}\)任何一个成功提交了，另外一个即过期了(因为nonce是一样的)，这样也可以达到终止的目的。</p>
</li>
<li><p><strong>实验模拟的三种情况</strong><br>1、交易在规定时间内没有被打包<br>2、因为手续费不足，用户决定撤回之前发布的交易<br>3、因为账户余额不足，交易无法被正常提交，即进入”pending”状态</p>
</li>
<li><p><strong>对情况1的实验</strong><br>实验原理：为了降低交易被打包的概率，作者减少了交易的手续费，分别为市场均价的0%、10%、…、90%，并设置了10min的截止时间。如果截止时间内原交易未提交，就按照上文提到的竞争法进行终止。<br>实验结果1：大部分交易都成功提交了，甚至30%-90%市场价的交易全部成功提交。出现这样的情况，我猜测可能是因为设置的终止时间太长了，即虽然手续费低，但只要时间足够长，交易还是有可能成功提交的。<br>实验结果2：在0-20%市场价的交易中，有16个没有成功提交，但最后都成功终止了，终止成功率为100%。</p>
</li>
<li><p><strong>对情况2的实验</strong><br>实验原理：这种情况考虑的是客户端希望撤回原先的交易，因此相比情况1，设置的截止时间要短一些（因为如果再设置很长时间的话，就失去模拟用户自行撤回的意义了），实验中是取所有交易提交时间的中位点，即3min。<br>实验结果1：截止时间缩短后，未提交的交易数明显增多，总共有53个交易没有提交，甚至0-20%市场价的交易全部没有提交成功，这也验证了我在实验1中的猜想。<br>实验结果2：虽然未提交的交易数量大大增加，但依旧每个交易都成功终止了，成功率为100%。</p>
</li>
<li><p><strong>对情况3的实验</strong><br>实验原理：首先作者创建了两个交易：\(Tx_1(bonce=n+1,value=\frac {k} {1000})\)和\(Tx_1(bonce=n+1,value=\frac {999k} {1000})\)，k为当前账户的余额。其次为了模拟余额不足这种情况，作者采用了一种很巧妙的方式：先发送\(Tx_2\)，隔5s后再发送\(Tx_1\)，这样可以使得\(Tx_2\)顺利发出，因为先发\(Tx_1\)的话，一旦其被顺利打包，\(Tx_2\)就会因为余额不够而无法发送到网络中了。采用这种方式，\(Tx_2\)就有可能比\(Tx_1\)先发到网络中，又因为\(Tx_1 nonce&lt;Tx_2 nonce\)，所以\(Tx_1\)必然会比\(Tx_2\)先打包，这样\(Tx_2\)就会因余额不够而停滞在网络中，即成功模拟了因余额不足所导致的”pending”状态。<br>实验结果：作者进行了100次实验，终止成功率依然是100%，结合前两次实验的结果，说明终止机制的效果还是非常好的。</p>
</li>
</ul>
<h2 id="个人感悟"><a href="#个人感悟" class="headerlink" title="个人感悟"></a>个人感悟</h2><h3 id="对于区块链的理解"><a href="#对于区块链的理解" class="headerlink" title="对于区块链的理解"></a>对于区块链的理解</h3><ul>
<li>区块链的本质是一个去中心化的分布式账本。我认为区块链就是一个账本，记载了所有交易信息和资产信息，但和传统金融记录方式的不同在于，区块链账本并非存储在某一个中心，其在每一个用户的本地都有存储的副本，相当于每个用户都是一个中心。  </li>
<li>去中心化带来的好处：首先是不用考虑中心的故障问题，就像双11，你再想买一个东西，淘宝服务器一旦瘫痪就不行了，所有的交易都依赖于这个第三方的中心；其次是不用担心信息泄露的问题，还是淘宝的例子，你要买东西、卖家要卖东西，都要提供个人信息，交易完成后也会产生交易信息，这些都被存在淘宝的服务器中，在当今这个强调个人隐私的时代，这是非常让人困扰的；第三是安全，区块链技术具有不可篡改性，我们不用担心因第三方中心的原因导致我们的资产受损失(如银行破产)。  </li>
<li>去中心化带来的问题：最大的问题是我们如何在各个节点间同步数据。在中心化系统中，中心说什么就是什么，其他人只需和中心保持同步即可；而在去中心化系统中，我们需要和全网进行同步，难度增大了很多，为了解决这个问题，也就出现了共识机制，大家共同记账，一定数量的节点达成共识后，就可正式在网络中进行同步了；而为了提高大家记账的积极性，就出现了工作量证明算法，对于成功记账的人会有奖励，大家都去积极记账了，账本自然就可以不断地维护、更新了。</li>
</ul>
<h3 id="对于区块链可靠性的理解"><a href="#对于区块链可靠性的理解" class="headerlink" title="对于区块链可靠性的理解"></a>对于区块链可靠性的理解</h3><ul>
<li>本文对于区块链可靠性的研究主要围绕“交易提交时间”这个概念，一个交易产生了，最好的情况是在规定时间内顺利打包，加入到区块链主链中。但因为复杂的网络环境、交易手续费或其他种种因素，一个交易的提交很可能会产生延迟，甚至会出现交易既没有提交、又没有销毁的”pending”状态。  </li>
<li>这就需要我们去探讨两个方面的问题：到底有哪些因素会影响交易的提交时间？交易如果没有顺利提交我们应该采取什么办法？前者就是文章第三、第四节主要研究的内容，通过具体的实验分析了各种可能影响因素；后者则是作者在第六节研究的内容，提出了一种交易终止机制并进行了实验测试。</li>
</ul>
<h3 id="对于交易终止机制的理解"><a href="#对于交易终止机制的理解" class="headerlink" title="对于交易终止机制的理解"></a>对于交易终止机制的理解</h3><ul>
<li>文中提到了两种交易终止的方法：竞争法和重传法，虽然有些区别，但本质思想都是一样的，都是利用了“nonce相等”的原理来终止原来的交易。  </li>
<li>文中还提到，虽然以太坊不像比特币那样把每一个块都链接起来，但其每个块都有唯一的顺序号nonce，而且主链上的区块必须是按顺序的，因此我们就可以利用nonce号来使某次交易过期，从而达到终止的目的。</li>
</ul>
<h3 id="其他一些感想"><a href="#其他一些感想" class="headerlink" title="其他一些感想"></a>其他一些感想</h3><ul>
<li>论文中还有一点使我印象很深刻，就是整篇文章都贯穿着实验的思想，作者进行了大量的实验去研究某些因素的影响，对所提出的终止机制也进行了具体的实验探究，实验过后还有对实验结果的合理分析，这些都使得整篇文章的逻辑结构很清晰，值得我在今后的科研工作中去学习。</li>
</ul>
<h2 id="改进方案"><a href="#改进方案" class="headerlink" title="改进方案"></a>改进方案</h2><h3 id="对比特币实验中无序现象产生的原因进行研究"><a href="#对比特币实验中无序现象产生的原因进行研究" class="headerlink" title="对比特币实验中无序现象产生的原因进行研究"></a>对比特币实验中无序现象产生的原因进行研究</h3><ul>
<li>在第三节中，作者研究了到达顺序对交易提交时间的影响，最终得出了结论：顺序到达的交易比无序到达的交易的延时要短。</li>
<li>作者虽然分析出了现象，但对产生这种现象的原因并没有深入研究，我认为如果对无序现象产生的原因进行研究，也会有助于改善区块链系统的可靠性问题。</li>
<li>方案实现：可参考文中第三节引言部分提到的几种情况，我认为可以主要研究节点转发策略和节点负载这两个因素，转发策略的不同可能会打乱原先的交易分发顺序，从而使得交易无法按顺序到达其他节点；而节点负载主要会影响交易传播的速度，不同节点的传播速度如果不同，也可能会导致交易失序。有了初步的分析，我们可设计实验来具体研究：首先选择一定量的矿工节点，将其按照不同的转发策略和不同的节点负载进行分组，转发策略可通过观察客户端版本、研究底层转发行为的实现代码等方式得出，而节点负载可通过观测相应网络指标得到。之后可以对两种因素进行控制变量分析，从而研究其对交易到达顺序的影响。</li>
</ul>
<h3 id="对比特币实验2中部分Orphans数据未统计的情况进行改进"><a href="#对比特币实验2中部分Orphans数据未统计的情况进行改进" class="headerlink" title="对比特币实验2中部分Orphans数据未统计的情况进行改进"></a>对比特币实验2中部分Orphans数据未统计的情况进行改进</h3><ul>
<li>在比特币的实验2中，因为高网络负载的原因，有20%的Orphans交易最后未被打包到块中，观测样本因此少了20%，作者也认为这对实验结果是一个很大的限制，因此我认为可根据这一点做下改进。  </li>
<li>方案实现：我考虑了两种思路，思路一：选择网络负载比实验2小但比实验1大的一个时间点，按照同样的方式做实验3，这样既可观测到不同网络环境下的实验结果，也可以减小网络负载对Orphans交易的影响，不至于使实验观测到的样本减少太多；思路二：保持实验1和实验2的网络条件不变，增加观测窗口的时长，分别进行实验3和实验4。由Fig.2可知计算的是累积比例，因此增加观测窗口相当于延长了一段X轴，Orphans Exp 2这条线最终就有机会到达1，而对其他三条曲线则不会有影响(它们已经很接近1了)，因此就可收集到更多的Orphans数据。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://yoursite.com/2018/09/13/Paper Notes/On Availability For Blockchain based Systems/" data-id="cjqqhv6j1002k3hqxg5o7zhsl" class="article-share-link" data-share="baidu" data-title="On Availability For Blockchain based Systems读书笔记">Share</a>
      

      
        <a href="http://yoursite.com/2018/09/13/Paper Notes/On Availability For Blockchain based Systems/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Paper-Notes/">Paper Notes</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Algorithm/MOOC_C语言程序设计笔记" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/09/13/Algorithm/MOOC_C语言程序设计笔记/" class="article-date">
  <time datetime="2018-09-13T07:49:58.000Z" itemprop="datePublished">2018-09-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/09/13/Algorithm/MOOC_C语言程序设计笔记/">MOOC_C语言笔记</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="第一节-基础知识"><a href="#第一节-基础知识" class="headerlink" title="第一节_基础知识"></a>第一节_基础知识</h2><ul>
<li><strong>STL：</strong>C++中的标准模板库</li>
<li><p><strong>define和typedef</strong></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAXNUM 9999  <span class="comment">//宏定义，多用来替代常量</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="built_in">multimap</span>&lt;<span class="keyword">int</span>,<span class="keyword">int</span>&gt; MAP; <span class="comment">//为复杂的类型定义取别名，简化程序，也便于修改。</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>类型取值范围</strong></p>
</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:center">类型</th>
<th style="text-align:center">字节数</th>
<th style="text-align:center">取值范围</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">int</td>
<td style="text-align:center">4</td>
<td style="text-align:center">-2^31~2^31-1(-21亿~21亿)</td>
</tr>
<tr>
<td style="text-align:center">long long</td>
<td style="text-align:center">8</td>
<td style="text-align:center">-2^63~2^63-1(很大)</td>
</tr>
<tr>
<td style="text-align:center">float</td>
<td style="text-align:center">4</td>
<td style="text-align:center">3.4x10^-38~3.4x10^38(绝对值)</td>
</tr>
<tr>
<td style="text-align:center">double</td>
<td style="text-align:center">8</td>
<td style="text-align:center">1.7x10^-308~1.7x10^308(绝对值)</td>
</tr>
<tr>
<td style="text-align:center">char</td>
<td style="text-align:center">1</td>
<td style="text-align:center">-128~127</td>
</tr>
<tr>
<td style="text-align:center">bool</td>
<td style="text-align:center">1</td>
<td style="text-align:center">true/false</td>
</tr>
</tbody>
</table>
<ul>
<li><p>double类型比较相等：相减-&gt;是否小于一个很小的数(0.00001)。<br>比大小：直接&gt; &lt; 比较即可。</p>
</li>
<li><p>求字节数：sizeof(int)/sizeof(n)</p>
</li>
<li><p>反斜杠\，转义字符、win下地址路径；正斜杠/，网址路径。</p>
</li>
<li><p>字符型和整型数据可以相互转换，整型-&gt;字符型：只留最右边8位，再转成ASCII码。</p>
</li>
<li><p>常用ASCII码：<br>‘0’-‘9’：48-57<br>‘A’-‘Z’：65-90<br>‘a’-‘z’：97-122</p>
</li>
<li><p>十六进制常量：0x打头,0xFFA，1个十六进制位对应4个二进制位。<br>表示二进制：转化为16进制。00101011-&gt;0x2b</p>
</li>
<li><p>八进制整型常量：0打头，0677,1个八进制位对应3个二进制位。</p>
</li>
<li><p><strong>位运算</strong><br>单独对某些比特进行操作<br><strong>与&amp;</strong>：将某些位置0、获取变量中的某一位<br>eg：判断n的第7位是否为为0，n&amp;0x80==ox80？</p>
</li>
</ul>
<p><strong>或|</strong>：将某些位置1</p>
<p><strong>异或^</strong>：相同为0不同为1。将某些位按位取反，其他位不变：取反的与1异或，其他与0异或。</p>
<p><strong>非~</strong>：按位取反。</p>
<p><strong>左移&lt;&lt;</strong>：左移n位即乘2的n次方，但比乘法要快很多。</p>
<p><strong>右移&gt;&gt;</strong>：除2的n次方，且向小取整。高位补符号位。</p>
<h2 id="第二节-数据结构"><a href="#第二节-数据结构" class="headerlink" title="第二节_数据结构"></a>第二节_数据结构</h2><h3 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h3><p>初始化：类型名 数组名[数量]={x1,x2,x3,x4..} 或 用for循环初始化。<br>数组越界编译器不会报错，会根据内存地址去访问，数组名即相当于内存地址。<br><strong>编程技巧：可用数组取代复杂的switch case分支结构</strong></p>
<h3 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h3><ul>
<li><strong>字符串三种形式</strong><br>1、字符串常量，双引号括起来””。<br>2、存放在char数组中，以’\0’结尾，多占一个数组元素。<br>3、string对象，c++标准模板库中的类。<br>注：char数组比string快，但char数组没法传字符串值，只能传数组首地址。因此在一些需要传值的场合，用string更合适。</li>
</ul>
<p>字符串在内存占的字节数等于字符数目加1(‘\0’的存在 )<br><strong>读入字符串</strong>：cin或scanf，读到空格为止。空格后内容在下一次cin读入。<br><strong>读一行</strong>：cin.getline(char buf[],int bufSize)，读入不超过bufsize-1个字符</p>
<h3 id="结构体struct"><a href="#结构体struct" class="headerlink" title="结构体struct"></a>结构体struct</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Student</span>&#123;</span></span><br><span class="line">    <span class="keyword">unsigned</span> ID;</span><br><span class="line">    <span class="keyword">char</span> name[<span class="number">10</span>];</span><br><span class="line">    <span class="keyword">float</span> fGPA;</span><br><span class="line">    Student* frd;<span class="comment">//成员可以是结构体的指针</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">Student s1 = &#123;<span class="number">1234</span>,<span class="string">"TOM"</span>,...&#125;;<span class="comment">//初始化</span></span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;s1.ID;</span><br><span class="line">Student* s2;</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;s2-&gt;ID; / <span class="built_in">cout</span>&lt;&lt;(*s2).ID;指针访问成员</span><br></pre></td></tr></table></figure>
<h3 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h3><p><strong>局部变量：</strong>定义在函数内部、语句块内部的变量。<br><strong>全局变量：</strong>定义在函数外部(main函数外)，在所有函数中均可使用。默认初始化为0.<br><strong>静态变量：</strong>全局变量和static定义的变量。生存期一直持续到整个程序结束。<br><strong>PS：</strong>若未初始化，静态变量默认赋值为0，非静态变量值为随机的。</p>
<h3 id="switch-case"><a href="#switch-case" class="headerlink" title="switch case"></a>switch case</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">switch</span>(表达式)&#123;</span><br><span class="line">    <span class="keyword">case</span> x1:</span><br><span class="line">    <span class="keyword">break</span>;  <span class="comment">//有无中括号都可以</span></span><br><span class="line">    <span class="keyword">case</span> x2:</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">    <span class="comment">//...</span></span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="第三节-函数与输入输出"><a href="#第三节-函数与输入输出" class="headerlink" title="第三节_函数与输入输出"></a>第三节_函数与输入输出</h2><h3 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">函数定义：</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fun</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">char</span> c)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//函数体</span></span><br><span class="line">    <span class="keyword">return</span> xx ;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">函数声明：(无函数体，常写在开头)</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fun</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">char</span> c)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fun2</span><span class="params">(<span class="keyword">int</span> a[])</span></span>;<span class="comment">//一维数组作形参，不用写大小。int a[]与int *a等价。</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fan3</span><span class="params">(<span class="keyword">int</span> a[][<span class="number">3</span>])</span><span class="comment">//二维数组作形参，行数不用写，列数必须写</span></span></span><br></pre></td></tr></table></figure>
<p>函数的形参是实参的<strong>拷贝</strong>，形参的改变不会影响实参。(除非形参是数组、引用或对象)<br>string类型也是传值<br>数组作形参传的是首地址，并非整个数组。</p>
<h3 id="printf、scanf格式化输入输出"><a href="#printf、scanf格式化输入输出" class="headerlink" title="printf、scanf格式化输入输出"></a>printf、scanf格式化输入输出</h3><p>比cin/cout效率高，尽量使用。<br>%：类型占位符</p>
<table>
<thead>
<tr>
<th style="text-align:center">格式字符</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">%d</td>
<td style="text-align:center">int类型</td>
</tr>
<tr>
<td style="text-align:center">%c</td>
<td style="text-align:center">char类型</td>
</tr>
<tr>
<td style="text-align:center">%f</td>
<td style="text-align:center">float类型</td>
</tr>
<tr>
<td style="text-align:center">%lf</td>
<td style="text-align:center">double类型</td>
</tr>
<tr>
<td style="text-align:center">%.xlf</td>
<td style="text-align:center">输出x位小数</td>
</tr>
</tbody>
</table>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;   //头文件</span></span></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"这是字符型数据%c，这是int型数据%d"</span>，ch,i);  <span class="comment">//格式化输出</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">scanf</span>(<span class="string">"%d%d"</span>, &amp;n, &amp;m);  <span class="comment">//记得加&amp;，返回值是接收变量的个数</span></span><br><span class="line"><span class="keyword">char</span> c[<span class="number">20</span>];</span><br><span class="line"><span class="built_in">scanf</span>(<span class="string">"%s"</span>,c); <span class="comment">//读入字符串，检测到/0结束</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//返回值为EOF(-1)时说明输入结束</span></span><br><span class="line"><span class="keyword">while</span>(<span class="built_in">scanf</span>(xxx) != EOF)&#123;</span><br><span class="line">    <span class="comment">//循环读入数据</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">cin</span>.peek()   <span class="comment">//看一个字符不取走</span></span><br><span class="line"><span class="built_in">cin</span>.putback(c)  <span class="comment">//把字符放回输入流头部</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">scanf</span>函数读空格和换行：</span><br><span class="line">如果是%d、%f数值类型，会自动跳过多余的空格和换行，如果是%c会读入空格和换行。</span><br></pre></td></tr></table></figure>
<h3 id="freopen重定向"><a href="#freopen重定向" class="headerlink" title="freopen重定向"></a>freopen重定向</h3><p>将输入由键盘重定向为文件(不用每次都输入测试数据)<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">freopen(<span class="string">"c:\\xxx.txt"</span>，<span class="string">"r"</span>,<span class="built_in">stdin</span>);</span><br><span class="line"><span class="keyword">while</span>(<span class="built_in">scanf</span>(...)!=EOF)&#123;</span><br><span class="line">    <span class="comment">//从txt文件中输入</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="cin函数"><a href="#cin函数" class="headerlink" title="cin函数"></a>cin函数</h3><p>cin &gt;&gt; n &gt;&gt; m;<br>返回True(成功接收所有输入)或false()<br>while(cin &gt;&gt; n){<br>    //循环读入数据<br>}<br>注：cin读入的格式由后面的变量决定，若为char即读入一个字符，int则读入一个整数(遇到空格/换行为止)</p>
<h2 id="第四节-指针"><a href="#第四节-指针" class="headerlink" title="第四节_指针"></a>第四节_指针</h2><ul>
<li><strong>指针</strong><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">指针变量，<span class="number">4</span>个字节，内容表示一个内存地址。</span><br><span class="line"><span class="keyword">int</span> *p; <span class="comment">//p类型：int *，*p的类型：int</span></span><br><span class="line"><span class="keyword">char</span> c1 = <span class="string">'A'</span>;</span><br><span class="line"><span class="keyword">char</span> *pc = &amp;c1; <span class="comment">//pc指向变量c1</span></span><br><span class="line"><span class="comment">//*：间接引用运算符。&amp;：取地址运算符。</span></span><br><span class="line"><span class="comment">//&amp;x：变量x的地址，就是指向x的指针，类型是 T*。</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>指针的意义：不需要通过变量，即可自由访问内存空间。</p>
<ul>
<li><p><strong>指针互相赋值</strong><br>不同类型的指针，如果不经过强制类型转换，不能直接相互赋值。 (每个指针类型还代表着一次向内存读/写多少字节)<br>char <em>c = ‘A’;<br>int </em>p = (int <em>)c;
</em>p=122; //此时’A’后面三个字节的值也会被改变。</p>
</li>
<li><p><strong>指针的运算</strong></p>
</li>
</ul>
<ol>
<li>指针比大小：比p1和p2地址的大小</li>
<li>指针相减：p1-p2=(地址p1-地址p2)/sizeof(T)</li>
<li>p+n=p+n*sizeof(T)，结果还是指针。</li>
<li>p[n]=*(p+n)</li>
</ol>
<ul>
<li><p><strong>空指针</strong><br>指向地址0的指针，int *pn=NULL;</p>
</li>
<li><p><strong>指针和字符串</strong><br>字符串常量、字符数组的类型都是char*。</p>
</li>
<li><p><strong>void指针</strong></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">可以用任何类型的指针对<span class="keyword">void</span>指针进行赋值</span><br><span class="line"><span class="keyword">int</span> a = <span class="number">3</span>;</span><br><span class="line"><span class="keyword">void</span> *p = &amp;a;</span><br><span class="line">但*p、P+n、p++等均无意义</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>void*自动匹配</strong></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">memset</span>函数</span><br><span class="line">void* memset(void* dest,int ch,int n)：将dest开始的n个字节都设置为ch(取其最低位字节)，初始化数组。</span><br><span class="line"><span class="comment">//对char数组赋值时，结尾要加'\0'</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">memcpy</span>函数</span><br><span class="line">void* memcpy(void* dest,void* src,int n)：将n个字节拷贝src-&gt;dest。</span><br><span class="line"></span><br><span class="line"><span class="keyword">char</span> a1[<span class="number">10</span>]=<span class="string">""</span>;</span><br><span class="line"><span class="keyword">char</span> a2[<span class="number">10</span>]=<span class="string">""</span>;</span><br><span class="line"><span class="built_in">memset</span>(a1, <span class="string">'a'</span>, <span class="keyword">sizeof</span>(a1)<span class="number">-1</span>);</span><br><span class="line"><span class="built_in">memcpy</span>(a2, a1, <span class="keyword">sizeof</span>(a2));</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; a2 &lt;&lt; <span class="built_in">endl</span>; <span class="comment">//输出aaaaaaaaaa</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>函数指针</strong></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//指向函数入口地址的指针</span></span><br><span class="line"><span class="keyword">int</span> (*Pf)(<span class="keyword">int</span>,<span class="keyword">char</span>)</span><br><span class="line">pf = fun_name;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="第五节-基本STL"><a href="#第五节-基本STL" class="headerlink" title="第五节_基本STL"></a>第五节_基本STL</h2><ul>
<li><strong>库函数和头文件</strong><br>库函数：编译器自带的函数<br>头文件：包含许多库函数的声明</li>
</ul>
<h3 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">//排序</span></span><br><span class="line"><span class="comment">//sort(数组名+n1,数组名+n2)，对数组[n1,n2)区间排序，默认从小到大</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> a[] = &#123;<span class="number">15</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">9</span>,<span class="number">2</span>,<span class="number">6</span>&#125;</span><br><span class="line">sort(a,a+<span class="keyword">sizeof</span>(<span class="keyword">int</span>)/<span class="keyword">sizeof</span>(<span class="keyword">int</span>))  <span class="comment">//从小到大排序</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//从大到小排序</span></span><br><span class="line"><span class="comment">//#include &lt;functional&gt;头文件</span></span><br><span class="line">sort(a,a+<span class="number">7</span>,greater&lt;T&gt;())  </span><br><span class="line"></span><br><span class="line"><span class="comment">//自定义排序规则</span></span><br><span class="line"><span class="keyword">bool</span> cmp(<span class="keyword">const</span> <span class="keyword">int</span> &amp; a1,<span class="keyword">const</span> <span class="keyword">int</span> &amp; a2) &#123;</span><br><span class="line">    <span class="keyword">return</span> a1 &gt; a2;</span><br><span class="line">    <span class="comment">//若a1应排在a2前面，则返回true。否则返回false</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">int</span> a[] = &#123;<span class="number">15</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">9</span>,<span class="number">2</span>,<span class="number">6</span>&#125;</span><br><span class="line">sort(a,a+<span class="keyword">sizeof</span>(a)/<span class="keyword">sizeof</span>(<span class="keyword">int</span>),cmp());</span><br></pre></td></tr></table></figure>
<h3 id="二分查找"><a href="#二分查找" class="headerlink" title="二分查找"></a>二分查找</h3><ul>
<li><strong>前提条件</strong><br>所有的二分查找，都是应用在<strong>有序</strong>列表中的。</li>
<li><p><strong>binary_search函数</strong><br>两种使用方法：<br><code>bool binary_search(数组名+n1,数组名+n2,值)</code><br><code>bool binary_search(数组名+n1,数组名+n2,值,排序规则结构名())</code>，查找时的规则必须和排序时的规则一致。<br>对排好序的数组进行二分查找，返回值true：找到，false：没找到。<br><strong>查找的本质：</strong>查找x，即找到一个元素y，使得“x必须排在y前面”和“y必须排在x前面”都不成立（和 == 的含义不一样），这样就算找到了；没有找到元素y，就是没找到。<br>所以不存在的元素同样有可能查的到（eg：按个位排序，查找的元素只要个位相等就能查到，十位/百位不一定会相等），因此查找的结果要视具体排序规则来分析。</p>
</li>
<li><p><strong>lower_bound函数</strong><br><code>T* lower_bound(数组名+n1,数组名+n2,值)</code><br>返回序号p，使得[n1,p)中元素都小于查找值。<br>若比所有元素都小/大，则指向n1/n2。<br>为避免返回序号为n1时的歧义，查前要确保值在数组范围内。</p>
</li>
<li><p><strong>upper_bound函数</strong><br><code>T* upper_bound(数组名+n1,数组名+n2,值)</code><br>返回指针p，使得[p,n2)中元素都大于查找值。</p>
</li>
</ul>
<h3 id="平衡二叉树"><a href="#平衡二叉树" class="headerlink" title="平衡二叉树"></a>平衡二叉树</h3><p>在log(n)内快速添加、删除、查找元素<br>应用了平衡二叉树的四种<strong>排序容器</strong>：<code>multiset</code>、<code>set</code>、<code>multimap</code>、<code>map</code></p>
<h3 id="multiset容器"><a href="#multiset容器" class="headerlink" title="multiset容器"></a>multiset容器</h3><ul>
<li><p><strong>主要作用</strong><br>自动对集合内元素排序(默认从小到大)，且允许元素重复，在一些需要动态增加、删除元素的排序场景下使用很方便。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;set&gt;  //头文件</span></span></span><br><span class="line"><span class="built_in">multiset</span>&lt;T&gt; st;</span><br><span class="line">T a[<span class="number">5</span>] = &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>&#125;</span><br><span class="line"><span class="built_in">multiset</span>&lt;T&gt; st2(a,a+<span class="number">5</span>); <span class="comment">//用数组初始化</span></span><br><span class="line">st.insert(a); <span class="comment">//插入元素a，自动排序。插入的是复制值，并非引用。</span></span><br><span class="line">st.erase(a);  <span class="comment">//删除元素a</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//遍历集合，用迭代器</span></span><br><span class="line"><span class="built_in">multiset</span>&lt;<span class="keyword">int</span>&gt;::iterator it;  <span class="comment">//定义迭代器，类似指针</span></span><br><span class="line"><span class="keyword">for</span> (it = st.begin(); it != st.end(); it++) &#123;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; *it &lt;&lt; <span class="string">" "</span>;</span><br><span class="line">    <span class="comment">//end()为末尾指针，指向最后一个元素的后面</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">it = st.find(a);   <span class="comment">//查找元素a，返回迭代器,没找到返回end()</span></span><br><span class="line"></span><br><span class="line">st.lower_bound(a);  <span class="comment">//返回迭代器it,使得[begin(),it)中元素都比a小，注意是前闭后开区间</span></span><br><span class="line"></span><br><span class="line">st.upper_bound(a)  <span class="comment">//返回迭代器it,使得[it,end())中元素都比a大</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>自定义排序规则的multiset</strong><br>1、自定义规则结构体Rule，实现<code>bool operator()(const &amp; T,const &amp; T)</code>函数<br>2、定义容器multiset&lt;T,Rule&gt;<br>3、定义迭代器muliset&lt;T,Rule&gt;::iterator</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//自定义规则结构体Rule方法</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Rule</span>&#123;</span></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="keyword">const</span> &amp; T,<span class="keyword">const</span> &amp; T)</span></span>&#123;</span><br><span class="line">        <span class="comment">//比较函数</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="set容器"><a href="#set容器" class="headerlink" title="set容器"></a>set容器</h3><p>与multiset容器区别：不能有重复元素，其他都一样。<br>a和b重复的含义：a排在b前面、b排在a前面都不成立<br>注：因为不能重复，所以插入元素有可能不成功。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pair&lt;<span class="built_in">set</span>&lt;T&gt;::iterator,<span class="keyword">bool</span>&gt; result = <span class="built_in">set</span>.insert(n);  </span><br><span class="line"><span class="comment">//result.second==true，插入成功；否则失败。</span></span><br></pre></td></tr></table></figure></p>
<h3 id="multimap容器"><a href="#multimap容器" class="headerlink" title="multimap容器"></a>multimap容器</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">键值对形式，<span class="built_in">multimap</span>中的元素都是pair形式。</span><br><span class="line">按first进行排序，一般不自定义排序。</span><br><span class="line"></span><br><span class="line">pair模板：pair&lt;T1,T2&gt;</span><br><span class="line">等价于</span><br><span class="line"><span class="class"><span class="keyword">struct</span>&#123;</span></span><br><span class="line">    T1 first;  <span class="comment">//关键字</span></span><br><span class="line">    T2 second; <span class="comment">//值 </span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;map&gt;</span></span></span><br><span class="line"><span class="built_in">multimap</span>&lt;T1,T2&gt; mp;</span><br><span class="line">mp.insert(make_pair(T1变量,T2变量))  <span class="comment">//make_pair为转换pair模板的函数</span></span><br><span class="line">multima&lt;T1,T2&gt;::iterator it  <span class="comment">//迭代器</span></span><br></pre></td></tr></table></figure>
<h3 id="map容器"><a href="#map容器" class="headerlink" title="map容器"></a>map容器</h3><p>和multimap区别：不能有关键字重复的元素<br>可以使用[first]查找second值；若first不存在，则创建一个。<br>插入可能失败(first重复)。</p>
<h3 id="camath数学库"><a href="#camath数学库" class="headerlink" title="camath数学库"></a>camath数学库</h3><ol>
<li>abs(int x)：整数绝对值</li>
<li>fabs(double x)：浮点数绝对值</li>
<li>sqrt(double x)：求平方根</li>
<li>ceil(double x)：不小于x的最小整数(上取整)</li>
<li>sin(double x)/cos(double x)：x(弧度)的正/余弦</li>
</ol>
<h3 id="cstring字符串库"><a href="#cstring字符串库" class="headerlink" title="cstring字符串库"></a>cstring字符串库</h3><p>函数都是根据’\0’来判断字符串是否结束的。</p>
<ol>
<li>形参常为char c[]，实参可以为char数组或字符串常量。</li>
<li>int strcmp(char c1[],char c2[])：比较字符串，相等返回0，c1小返回负数。</li>
<li>char* strcpy(char dest[],char src[])：拷贝字符串src-&gt;dest，返回dest首地址。</li>
<li>int strlen(char c[])：求字符串长度</li>
<li>char<em> strchr(char </em>str,char c)：str中查字符c是否在str中，返回指向位置的指针，未查到为NULL。</li>
<li>char<em> strstr(char </em>str1,char *str2)：str1中查子串str2位置，返回指向位置的指针，未查到为NULL。</li>
<li>cbar<em> strtok(char </em>str1,char* str2)：str1中抽取被str2分隔的子串。(将str1中的分隔符用’/0’代替了)<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">char</span> *p = strtok(str1, str2);</span><br><span class="line">	<span class="keyword">while</span> (p != <span class="literal">NULL</span>) &#123;</span><br><span class="line">		<span class="built_in">cout</span> &lt;&lt; p &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">		p = strtok(<span class="literal">NULL</span>, str2);</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="ctype-h字符库"><a href="#ctype-h字符库" class="headerlink" title="ctype.h字符库"></a>ctype.h字符库</h3><ol>
<li>bool isdigit(int c)：判断c是否是数字字符</li>
<li>bool isalpha(int c)：判断c是否是字母字符</li>
</ol>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://yoursite.com/2018/09/13/Algorithm/MOOC_C语言程序设计笔记/" data-id="cjqqhv6ig001a3hqx5j26i3cf" class="article-share-link" data-share="baidu" data-title="MOOC_C语言笔记">Share</a>
      

      
        <a href="http://yoursite.com/2018/09/13/Algorithm/MOOC_C语言程序设计笔记/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithm/">Algorithm</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
  </nav>
</section>
      
      <aside id="sidebar">
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithm/">Algorithm</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/C/">C++</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/C-Sharp/">C_Sharp</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/English/">English</a><span class="tag-list-count">8</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Git/">Git</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/JSP/">JSP</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Java/">Java</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lingo/">Lingo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MachineLearning/">MachineLearning</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Network/">Network</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Other/">Other</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Paper-Notes/">Paper Notes</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SQL/">SQL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Swift/">Swift</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cs224n/">cs224n</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/uwtsd-modules/">uwtsd_modules</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/课程笔记/">课程笔记</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Algorithm/" style="font-size: 13.33px;">Algorithm</a> <a href="/tags/C/" style="font-size: 11.67px;">C++</a> <a href="/tags/C-Sharp/" style="font-size: 10px;">C_Sharp</a> <a href="/tags/English/" style="font-size: 20px;">English</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/JSP/" style="font-size: 10px;">JSP</a> <a href="/tags/Java/" style="font-size: 10px;">Java</a> <a href="/tags/Lingo/" style="font-size: 10px;">Lingo</a> <a href="/tags/MachineLearning/" style="font-size: 16.67px;">MachineLearning</a> <a href="/tags/Network/" style="font-size: 13.33px;">Network</a> <a href="/tags/Other/" style="font-size: 18.33px;">Other</a> <a href="/tags/Paper-Notes/" style="font-size: 13.33px;">Paper Notes</a> <a href="/tags/SQL/" style="font-size: 10px;">SQL</a> <a href="/tags/Swift/" style="font-size: 10px;">Swift</a> <a href="/tags/cs224n/" style="font-size: 15px;">cs224n</a> <a href="/tags/python/" style="font-size: 13.33px;">python</a> <a href="/tags/uwtsd-modules/" style="font-size: 10px;">uwtsd_modules</a> <a href="/tags/课程笔记/" style="font-size: 10px;">课程笔记</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">June 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a><span class="archive-list-count">26</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/12/25/Python/numpy笔记/">numpy笔记</a>
          </li>
        
          <li>
            <a href="/2018/11/26/cs224n/lecture 3/">lecture 2_Morew Word Vectors</a>
          </li>
        
          <li>
            <a href="/2018/11/06/cs224n/background/">基础知识</a>
          </li>
        
          <li>
            <a href="/2018/11/05/cs224n/lecture 2/">lecture 2_Word Vectors</a>
          </li>
        
          <li>
            <a href="/2018/11/02/cs224n/lecture 1/">lecture 1_Introduction</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Links</h3>
    <div class="widget">
      <ul>
        
      </ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Ren Li<br>
      Powered by <a href="//hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/xiangming/landscape-plus" target="_blank">Landscape-plus</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
  <!-- totop start -->
<div id="totop">
<a title="totop"><img src="/img/scrollup.png"/></a>
</div>

<!-- totop end -->

<!-- 多说公共js代码 start -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"reqianduan"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
     || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
  </script>
<!-- 多说公共js代码 end -->


<!-- 百度分享 start -->

<div id="article-share-box" class="article-share-box">
  <div id="bdshare" class="bdsharebuttonbox article-share-links">
    <a class="article-share-weibo" data-cmd="tsina" title="分享到新浪微博"></a>
    <a class="article-share-weixin" data-cmd="weixin" title="分享到微信"></a>
    <a class="article-share-qq" data-cmd="sqq" title="分享到QQ"></a>
    <a class="article-share-renren" data-cmd="renren" title="分享到人人网"></a>
    <a class="article-share-more" data-cmd="more" title="更多"></a>
  </div>
</div>
<script>
  function SetShareData(cmd, config) {
    if (shareDataTitle && shareDataUrl) {
      config.bdText = shareDataTitle;
      config.bdUrl = shareDataUrl;
    }
    return config;
  }
  window._bd_share_config={
    "common":{onBeforeClick: SetShareData},
    "share":{"bdCustomStyle":"/css/bdshare.css"}
  };
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?cdnversion='+~(-new Date()/36e5)];
</script>

<!-- 百度分享 end -->

<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>




<script src="/js/script.js"></script>

</div>
</body>
</html>
