
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Ren Li&#39;s blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Ren Li&#39;s blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Ren Li&#39;s blog">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Ren Li&#39;s blog">
  
    <link rel="alternative" href="/atom.xml" title="Ren Li&#39;s blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" href="/css/style.css">
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
</head>
<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Ren Li&#39;s blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Think and write down</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" class="search-form">
          <input type="search" name="word" maxlength="20" class="search-form-input" placeholder="Search">
          <input type="submit" value="" class="search-form-submit">
          <input name=tn type=hidden value="bds">
          <input name=cl type=hidden value="3">
          <input name=ct type=hidden value="2097152">
          <input type="hidden" name="si" value="yoursite.com">
        </form>
      </div>
    </div>
  </div>
</header>
    <div class="outer">
      <section id="main">
  
    <article id="post-Regular Expression" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/02/07/Regular Expression/" class="article-date">
  <time datetime="2019-02-07T12:10:58.222Z" itemprop="datePublished">2019-02-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/02/07/Regular Expression/">正则表达式</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script>

<p>因为正则表达式的在文本处理，字符串匹配中有着重要应用，因此本文对其基本语法规则及在Python中的应用进行了简要介绍。<br>注：本篇博文大部分是对博文<a href="http://deerchao.net/tutorials/regex/regex.htm" target="_blank" rel="noopener">正则表达式30分钟入门教程</a>和<a href="http://lizec.top/2017/08/08/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E7%AC%94%E8%AE%B0/" target="_blank" rel="noopener">正则表达式笔记</a>的归纳和整理，更详细内容可查阅原文。</p>
<h2 id="基本语法"><a href="#基本语法" class="headerlink" title="基本语法"></a>基本语法</h2><ul>
<li><strong>基本概念：</strong> 正则表达式(regular expression)描述了一种字符串匹配的模式（pattern），可以用来匹配、检测、替换特定的字符串等。</li>
</ul>
<h3 id="字符"><a href="#字符" class="headerlink" title="字符"></a>字符</h3><ul>
<li><strong>普通字符：</strong>通常正则表达式中出现的任意一个字符代表匹配和他们一样的字符。</li>
<li><strong>特殊字符：</strong>特殊字符并不匹配他们本身，而是有特殊含义的，具体有：<code>. ^ $ * + ? { } [ ] \ | ( )</code><br>若要匹配这些字符本身，则需要加上<code>\</code>进行转义（escape），如<code>\*</code>表示匹配<code>*</code>本身。</li>
</ul>
<h3 id="特殊字符含义"><a href="#特殊字符含义" class="headerlink" title="特殊字符含义"></a>特殊字符含义</h3><table>
<thead>
<tr>
<th style="text-align:center">字符</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">.</td>
<td style="text-align:center">匹配任意单个字符(除换行符“\n”外)</td>
</tr>
<tr>
<td style="text-align:center">\w</td>
<td style="text-align:center">匹配任意字母、数字、下划线、汉字（即匹配普通字符）</td>
</tr>
<tr>
<td style="text-align:center">\s</td>
<td style="text-align:center">匹配任意的空白符（空格、制表符(Tab)、换行符、中文全角空格等）</td>
</tr>
<tr>
<td style="text-align:center">\d</td>
<td style="text-align:center">匹配数字（0-9）</td>
</tr>
<tr>
<td style="text-align:center">\b</td>
<td style="text-align:center">匹配单词的开始或结束（单词边界），即<strong>非字母数字的任意其他字符</strong></td>
</tr>
<tr>
<td style="text-align:center">^</td>
<td style="text-align:center">匹配字符串的开始？</td>
</tr>
<tr>
<td style="text-align:center">$</td>
<td style="text-align:center">匹配字符串的结束？</td>
</tr>
</tbody>
</table>
<ul>
<li><p>关于单词边界<code>\b</code>：如对于pattern <code>\blove\b</code>，就会匹配句子”I love you”中的love, 而不会匹配”I aloveb you”中的love。</p>
</li>
<li><p>一些字符的反义表示<br>如<code>\w</code>，<code>\b</code>这些特殊字符的大写表示其相反的含义，具体如下：</p>
</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:center">字符</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">\W</td>
<td style="text-align:center">匹配不是字母、数字、下划线、汉字的字符（多用来匹配特殊字符）</td>
</tr>
<tr>
<td style="text-align:center">\S</td>
<td style="text-align:center">匹配任意不是空白符的字符</td>
</tr>
<tr>
<td style="text-align:center">\D</td>
<td style="text-align:center">匹配任意非数字的字符</td>
</tr>
<tr>
<td style="text-align:center">\B</td>
<td style="text-align:center">匹配不是单词开头或结束的位置</td>
</tr>
</tbody>
</table>
<h3 id="重复匹配限定符"><a href="#重复匹配限定符" class="headerlink" title="重复匹配限定符"></a>重复匹配限定符</h3><p>以下字符称为“限定符”（限制次数的符号），跟在特殊字符的<strong>后面</strong>，表示重复该字符特定次数。<br>如<code>\d+</code>匹配1个或更多个的数字。</p>
<table>
<thead>
<tr>
<th style="text-align:center">限定符</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">*</td>
<td style="text-align:center">重复\(\geq\)0次</td>
</tr>
<tr>
<td style="text-align:center">+</td>
<td style="text-align:center">重复\(\geq\)1次</td>
</tr>
<tr>
<td style="text-align:center">?</td>
<td style="text-align:center">重复0次或1次</td>
</tr>
<tr>
<td style="text-align:center">{n}</td>
<td style="text-align:center">重复n次</td>
</tr>
<tr>
<td style="text-align:center">{n,}</td>
<td style="text-align:center">重复\(\geq\)n次</td>
</tr>
<tr>
<td style="text-align:center">{n,m}</td>
<td style="text-align:center">重复n到m次</td>
</tr>
</tbody>
</table>
<h4 id="贪婪、懒惰匹配原则"><a href="#贪婪、懒惰匹配原则" class="headerlink" title="贪婪、懒惰匹配原则"></a>贪婪、懒惰匹配原则</h4><ul>
<li>正则表达式的匹配原则为贪婪匹配：即在满足条件的情况下，匹配<strong>尽可能多</strong>的字符。<br>eg. <code>a.*b</code>会匹配以a开始，以b结束的最长的字符串。如果用来搜索<code>aabab</code>，则会匹配整个字符串<code>aabab</code>而非<code>aab</code>。</li>
<li>若需要懒惰匹配：匹配<strong>尽可能少</strong>的字符，就在相应的重复字符<strong>后</strong>加<code>?</code>即可。<br>eg. <code>a.*?b</code>匹配以a开始，以b结束的最短的字符串，搜索<code>aabab</code>会匹配到<code>aab</code>和<code>ab</code>两个子串（之所以没有仅仅匹配到<code>ab</code>子串，是因为正则表达式的匹配还会考虑开始的先后顺序，最开始匹配的优先级最高）。</li>
<li>懒惰匹配限定符如下：</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:center">限定符</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">*?</td>
<td style="text-align:center">重复\(\geq\)0次，但尽可能少重复</td>
</tr>
<tr>
<td style="text-align:center">+?</td>
<td style="text-align:center">重复\(\geq\)1次，但尽可能少重复</td>
</tr>
<tr>
<td style="text-align:center">??</td>
<td style="text-align:center">重复0次或1次，但尽可能少重复</td>
</tr>
<tr>
<td style="text-align:center">{n,}?</td>
<td style="text-align:center">重复\(\geq\)n次，但尽可能少重复</td>
</tr>
<tr>
<td style="text-align:center">{n,m}?</td>
<td style="text-align:center">重复n到m次，但尽可能少重复</td>
</tr>
</tbody>
</table>
<h3 id="字符类"><a href="#字符类" class="headerlink" title="字符类"></a>字符类</h3><ul>
<li>在 <code>[</code> 和 <code>]</code> 中的若干字符构成一个字符类(character class)。</li>
<li>字符类是为了匹配某种字符集合，表示此位置可以匹配这个类中的任意一个字符。</li>
<li>整个字符类所起到的作用和普通字符相同，都是只匹配<strong>单个字符</strong>，因此字符类可作为整体再接受其他限定，如<code>? + {n,m}</code>等等。</li>
<li>可以使用<code>-</code>来表示一个范围，例如<code>[a-c]</code>表示<code>[abc]</code></li>
<li>在字符类中的特殊符号不被转义</li>
<li><strong>反向匹配：</strong>在字符类中，如果以^开头，则表示匹配除此字符类中提及的任何其他字符。如<code>[^5]</code>匹配任何不是5的字符。</li>
</ul>
<h3 id="分枝条件"><a href="#分枝条件" class="headerlink" title="分枝条件"></a>分枝条件</h3><ul>
<li>在正则表达式表示“或”的逻辑，两个条件用<code>|</code>连接即可。<br>eg. <code>\d{5}-\d{4}|\d{5}</code>这个表达式用于匹配美国的邮政编码。美国邮编的规则是5位数字，或者用连字号间隔的9位数字。</li>
<li>在使用分枝条件时要注意各个条件的<strong>顺序</strong>，如果上文改成<code>\d{5}|\d{5}-\d{4}</code>的话，那么就只会匹配5位的邮编(以及9位邮编的前5位)。因为一旦前面的分枝满足的话就不会再管其他条件了。</li>
</ul>
<h3 id="注释"><a href="#注释" class="headerlink" title="注释"></a>注释</h3><p>不推荐在正则表达式内部写注视，注释推荐写在正则表达式的外部语言中。</p>
<h2 id="正则表达式高级特性"><a href="#正则表达式高级特性" class="headerlink" title="正则表达式高级特性"></a>正则表达式高级特性</h2><h3 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h3><ul>
<li>上文提到了如何重复单个字符（直接在字符后面加表示重复的限定符即可），但若想重复多个字符，就需要用到<strong>分组</strong>的概念。</li>
<li>用小括号<code>()</code>来指定分组（也叫子表达式），之后就可以对这个分组的整体进行重复或其他操作。<br>eg. <code>(\d{1,3}\.){3}\d{1,3}</code>，就是一个比较粗糙的IP地址匹配式。</li>
</ul>
<h3 id="后向引用"><a href="#后向引用" class="headerlink" title="后向引用"></a>后向引用</h3><ul>
<li>使用小括号指定一个分组后，这个分组可以作为一个整体在后文中作进一步处理。</li>
<li>默认情况下，每个组会有一个<strong>组号</strong>，从左到右，第一个出现的分组组号为1(注意不是0)，第二个为2，以此类推。</li>
<li>如何引用：在后问中使用<code>\</code>+组号的形式来引用，如<code>\1</code>表示引用1号分组的。<br>eg. <code>\b(\w+)\b\s+\1\b</code>可以来匹配如go go 或kitty kitty 这种连着两个重复单词。</li>
</ul>
<h3 id="零宽断言"><a href="#零宽断言" class="headerlink" title="零宽断言"></a>零宽断言</h3><ul>
<li>用于查找在某些内容前面或后面的东西（但不包括这些内容），类似于<code>^ $ \b</code>这种占位符，用于指定一个位置，这个位置应该满足一定的条件(即断言)，因此被称为零宽断言。</li>
<li><code>(?=exp)</code>：用于匹配exp前面出现的表达式。<br>e.g <code>\b\w+(?=ing\b)</code>用于匹配以ing为结尾的单词的<strong>前面部分</strong>（不包括ing）</li>
<li><code>(?&lt;=exp)</code>：用于匹配exp后面出现的表达式。<br>e.g <code>(?&lt;=\bre)\w+\b</code>用于匹配以re开头的单词的<strong>后面部分</strong>（不包括re）</li>
<li>更多示例：<br><code>(?&lt;=\s)\d+(?=\s)</code>：匹配以空白符间隔的数字(不包括这些空白符)<br><code>((?&lt;=\d)\d{3})+\b</code>：要给一个很长的数字中每三位间加一个逗号(当然是从右边加起了)，你可以这样查找需要在前面和里面添加逗号的部分（用它对<code>1234567890</code>进行查找时结果是<code>234567890</code>）</li>
</ul>
<h3 id="负向零宽断言"><a href="#负向零宽断言" class="headerlink" title="负向零宽断言"></a>负向零宽断言</h3><ul>
<li>用于确保某个模式不会出现。与字符类<code>[^exp]</code>的区别：虽然不匹配这个字符，但字符类总是会匹配某个字符的，这会限制字符类的应用场景；而负向零宽断言不匹配字符，其只指代一个位置。<br>eg. 如果用<code>\b\w*q[^u]\w*\b</code>来匹配“出现了字母q,但是q后面跟的不是字母u”的单词，则像“Iraq, Benq”这种q直接作为最后一个字符的情况就会出错（字符类总要匹配一个字符），因此就需要负向零宽断言。</li>
<li><code>(?!exp)</code>：断言此位置的后面不能匹配表达式exp。<br>eg.<code>\b((?!abc)\w)+\b</code>匹配不包含连续字符串abc的单词</li>
<li><code>(?&lt;!exp)</code>：断言此位置的前面不能匹配表达式exp。<br>eg. <code>(?&lt;![a-z])\d{7}</code>匹配前面不是小写字母的七位数字。</li>
</ul>
<h3 id="递归匹配"><a href="#递归匹配" class="headerlink" title="递归匹配"></a>递归匹配</h3><ul>
<li>用于匹配一些嵌套的层次结构，如<code>(100*(50+15))</code>，如果只是简单地使用<code>\(.+\)</code>则只会匹配到最左边的左括号和最右边的右括号之间的内容。假如原来的字符串里的左括号和右括号出现的次数不相等，比如<code>(5/(3+2)))</code>，那我们的匹配结果里两者的个数也不会相等。如果想要想匹配到<strong>最长的，而且配对正确的</strong>字符串，就需要用到递归匹配。</li>
<li>具体内容参见博文<a href="http://deerchao.net/tutorials/regex/regex.htm" target="_blank" rel="noopener">正则表达式30分钟入门教程</a></li>
</ul>
<h3 id="经典正则表达式实例"><a href="#经典正则表达式实例" class="headerlink" title="经典正则表达式实例"></a>经典正则表达式实例</h3><table>
<thead>
<tr>
<th style="text-align:left">正则表达式</th>
<th style="text-align:left">解释</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>^[A-Za-z]+$</code></td>
<td style="text-align:left">由26个字母组成的字符串</td>
</tr>
<tr>
<td style="text-align:left"><code>^[A-Za-z0-9]+$</code></td>
<td style="text-align:left">由26个字母和数字组成的字符串</td>
</tr>
<tr>
<td style="text-align:left"><code>^-?\d+$</code></td>
<td style="text-align:left">整数形式的字符串</td>
</tr>
<tr>
<td style="text-align:left"><code>^[0-9]*[1-9][0-9]*$</code></td>
<td style="text-align:left">正整数形式的字符串</td>
</tr>
<tr>
<td style="text-align:left"><code>[\u4e00-\u9fa5]</code></td>
<td style="text-align:left">判断是不是中文字符</td>
</tr>
</tbody>
</table>
<h2 id="在Python中使用正则表达式"><a href="#在Python中使用正则表达式" class="headerlink" title="在Python中使用正则表达式"></a>在Python中使用正则表达式</h2><h3 id="使用原生字符串"><a href="#使用原生字符串" class="headerlink" title="使用原生字符串"></a>使用原生字符串</h3><ul>
<li>正则表达式中使用<code>\n</code>表示转义，而python中恰好也使用<code>\n</code>表示转义，因此在python中使用正则表达式则需要<code>\\</code>来表示反斜杠。</li>
<li>为了节省过多的反斜杠，可以使用Python原生字符串特性，即在字符串开头加上<code>r</code>，如<code>r\d</code>，在这个字符串中每个字符表示其本身，Python不进行转义。</li>
</ul>
<h3 id="re库"><a href="#re库" class="headerlink" title="re库"></a>re库</h3><p>在Python中使用正则表达式直接导入<code>re</code>库即可 <code>import re</code>。</p>
<h4 id="常用函数介绍"><a href="#常用函数介绍" class="headerlink" title="常用函数介绍"></a>常用函数介绍</h4><table>
<thead>
<tr>
<th style="text-align:center">函数</th>
<th style="text-align:center">功能</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">re.compile()</td>
<td style="text-align:center">编译正则表达式，返回Regular Expression Objects对象</td>
</tr>
<tr>
<td style="text-align:center">re.search()</td>
<td style="text-align:center">在字符串中查找匹配的子串<strong>第一次出现</strong>的位置，匹配成功返回match对象否则返回None</td>
</tr>
<tr>
<td style="text-align:center">re.findall()</td>
<td style="text-align:center">在字符串中查找<strong>所有</strong>满足条件的子串，返回string list</td>
</tr>
<tr>
<td style="text-align:center">re.match()</td>
<td style="text-align:center">强制从起始位置开始匹配，匹配成功返回match对象否则返回 None</td>
</tr>
<tr>
<td style="text-align:center">re.split()</td>
<td style="text-align:center">将正则表达式作为separator来分割字符串，返回string list</td>
</tr>
<tr>
<td style="text-align:center">re.sub()</td>
<td style="text-align:center">在字符串中替换所有匹配正则表达式的子串，返回替换后的字符串</td>
</tr>
<tr>
<td style="text-align:center">re.escape()</td>
<td style="text-align:center">自动在字符串中添加 “\” 转义符（除去字母数字下划线），返回修改后的字符串 <br> 可用来自动生成包含特殊字符的正则表达式</td>
</tr>
</tbody>
</table>
<p><strong>参数说明：</strong></p>
<ul>
<li>re.compile(pattern, flags=0)<br>pattern: 表示正则表达式的字符串<br>flags: 正则表达式的控制标记</li>
<li>re.search(pattern, string, flags=0)<br>pattern：同上<br>string：待查找的字符串<br>flags: 同上</li>
<li>re.findall(pattern, string, flags=0)<br>参数同上</li>
<li>re.match(pattern, string, flags=0)<br>参数同上<br>re.fullmatch(pattern, string, flags=0)<br>参数同上，只不过是查找整个字符串</li>
<li>re.sub(pattern, repl, string, count=0, flags=0)<br>repl: 用来替代匹配到的子串的字符串</li>
<li>re.escape(pattern)<br>eg. <code>pattern1 = re.escape(&#39;python.exe&#39;))</code>，pattern1就为”python\.exe”，可直接作为pattern参数传入其他函数中，用来匹配“python.exe”。当pattern含有大量特殊字符时使用这个函数就很方便。</li>
</ul>
<h5 id="str-replace-和re-sub的比较"><a href="#str-replace-和re-sub的比较" class="headerlink" title="str.replace()和re.sub的比较"></a>str.replace()和re.sub的比较</h5><ul>
<li>str.replace(old, new[, count])是字符串的替代函数，其中new为替换的字符串，old为待替换的字符串，old只能为substring而不能为字符串，因此替换功能较为简单。</li>
<li>re.sub(pattern, repl, string, count=0, flags=0)则使用了正则表达式，可进行更复杂的替换操作，但同时开销也更大。</li>
<li>因此能用replace()尽量用，复杂的替换操作再使用正则表达式。</li>
</ul>
<h4 id="控制标记flag介绍"><a href="#控制标记flag介绍" class="headerlink" title="控制标记flag介绍"></a>控制标记flag介绍</h4><table>
<thead>
<tr>
<th style="text-align:left">标记名(简写/全称)</th>
<th style="text-align:left">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">re.A / re.ASCII</td>
<td style="text-align:left">使\w,\b,\s和\d只匹配ASCII字符 <br> eg. 不会匹配汉字和其他Unicode字符</td>
</tr>
<tr>
<td style="text-align:left">re.I / re.IGNORECASE</td>
<td style="text-align:left">忽略正则表达式大小写</td>
</tr>
<tr>
<td style="text-align:left">re.M / re.MULTILINE</td>
<td style="text-align:left">使得^表示每一行的开始，$表示每一行的结束 <br> 原义仅表示一个单词的开始和结束</td>
</tr>
<tr>
<td style="text-align:left">re.S / re.DOTALL</td>
<td style="text-align:left">使得.可以匹配\n字符</td>
</tr>
<tr>
<td style="text-align:left">re.X / re.VERBOSE</td>
<td style="text-align:left">忽略正则表达式内部的空白符</td>
</tr>
</tbody>
</table>
<p>注：这些变量在VSCode的python下没有提示，但可以运行。</p>
<h4 id="正则表达式的两种使用方法"><a href="#正则表达式的两种使用方法" class="headerlink" title="正则表达式的两种使用方法"></a>正则表达式的两种使用方法</h4><ul>
<li><p>直接函数调用：result = re.search(pattern, string)</p>
</li>
<li><p>先编译后使用：<br>prog = re.compile(pattern)<br>result = prog.search(string)</p>
</li>
<li><p>两种方式效果相同，且因为缓存机制，在正则表达式数量较少的的情况下，两者效率也相近；但如果重复地调用很多正则表达式，先编译好的效率会更高。</p>
</li>
</ul>
<h4 id="Match类"><a href="#Match类" class="headerlink" title="Match类"></a>Match类</h4><p>re.search()和re.match()两个函数的返回对象，包含了匹配的相关信息，常用函数和属性如下：</p>
<table>
<thead>
<tr>
<th style="text-align:left">名称</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">match.group()</td>
<td style="text-align:left">返回特定的分组，string或tuple的形式</td>
</tr>
<tr>
<td style="text-align:left">match.groups()</td>
<td style="text-align:left">以tuple的形式返回所有分组</td>
</tr>
<tr>
<td style="text-align:left">match.start()</td>
<td style="text-align:left">返回特定分组的起始地址</td>
</tr>
<tr>
<td style="text-align:left">match.end()</td>
<td style="text-align:left">返回特定分组的结束地址 <br>（会比实际大1位，因为python字符串截取前闭后开的特性）</td>
</tr>
<tr>
<td style="text-align:left">match.re</td>
<td style="text-align:left">为编译好的regular expression object对象</td>
</tr>
<tr>
<td style="text-align:left">match.string</td>
<td style="text-align:left">为传入到re.search()或re.match()的待匹配字符串</td>
</tr>
</tbody>
</table>
<p><strong>参数说明：</strong></p>
<ul>
<li>match.group([group1, …])<br>group1为组号，默认为0（这时返回the whole match）；当没有参数或只有一个参数时返回string，当有2个或以上参数时返回tuple。</li>
<li>match.groups(default=None)<br>default为没有匹配到的分组指定所显示的名字，通常保持默认即可。</li>
<li>match.start([group])、match.end([group])<br>两个函数中group都是指组号，返回特定组号的起始、终止地址；<br>因此相应分组的子串可通过<code>m.string[m.start(g):m.end(g)]</code>来获取。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://yoursite.com/2019/02/07/Regular Expression/" data-id="cjrziv5y1000gnkqxn5lpa5zv" class="article-share-link" data-share="baidu" data-title="正则表达式">Share</a>
      

      
        <a href="http://yoursite.com/2019/02/07/Regular Expression/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/nlp/">nlp</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-cs224n/lecture 4" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/01/27/cs224n/lecture 4/" class="article-date">
  <time datetime="2019-01-27T22:29:11.473Z" itemprop="datePublished">2019-01-27</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/01/27/cs224n/lecture 4/">lecture 4_Word Window Classification and Neural Networks</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><br>本节课是通过任务实例展开的，首先介绍了Word Window Classification任务，之后用softmax和cross-entropy进行线性分类，之后用神经网络进行了非线性分类（引入Max-Margin loss &amp; back propogation），其间对相关方法进行了讲解。</p>
<h2 id="Simple-word-classfication-task-——-linear-classfication"><a href="#Simple-word-classfication-task-——-linear-classfication" class="headerlink" title="Simple word classfication task —— linear classfication"></a>Simple word classfication task —— linear classfication</h2><ul>
<li>输入输出: \(\{x_i, y_i\}^N_{i=1}\)<br>\(x_i\): 词向量<br>\(y_i\): 输出的分类结果（各个类别的概率分布）</li>
<li>分类公式：<br><img src="/images/cs224n/4_Softmax classification.png" width="400" height="200" alt="Softmax Classification" align="center"><br>注：分类逻辑就相当于蕴含在矩阵\(W\)中</li>
<li>Loss function：<br>但光有分类公式还不够，我们还要使分类结果最优，即：使整个数据集的分类结果\(P(y|x)\)最优（一个\(P(y|x)\)只是针对单个词的分类结果）,因此就有了损失函数。</li>
<li>交叉熵：其中涉及到了Kullback-Leibler divergence（KL散度），其可用于衡量两个概率之间的误差，在这个任务中我们可以用来衡量\(P(y|x)\)和ground truth/gold/target probability（即正确的分类结果：[0,…0,1,0,…0]，一个为1，其余为0）的误差。误差越小，代表分类结果越好。</li>
<li>交叉熵公式：$$H(p, q) = -\sum_{c=1}^Cp(c)logq(c)$$<br>我们把p(c)看成gold，把q(c)看成预测结果，就有了我们最后的损失函数。</li>
<li>损失函数：<br>$$<br>J(\theta) = \frac {1} {N} \sum_{i=1}^N -log(\frac {exp(f_{yi})} {\sum_{c=1}^Cexp(f_c)})<br>$$<br>注1: \(f = Wx\)<br>注2: 最终最小化损失函数即可（Maxmize \(P(y|x)\) -&gt; Minimize \(-logP(y|x)\)）。<h3 id="分类问题中是否更新词向量"><a href="#分类问题中是否更新词向量" class="headerlink" title="分类问题中是否更新词向量"></a>分类问题中是否更新词向量</h3></li>
<li>在优化损失函数的时候，可以只选择\(W\)作为优化参数, 但可将词向量作为一个参数也同时进行梯度下降更新，从而使得词向量更为适合当前任务。</li>
<li>判断标准：当前任务的数据集的大小。若当前任务数据集不大，则更新词向量很有可能会contaminate原先的向量空间，使得效果反而下降；但若数据集总量很大，可以保证词向量优化效果，且计算资源充足，则应进行优化。</li>
</ul>
<h2 id="Word-Window-Classification"><a href="#Word-Window-Classification" class="headerlink" title="Word Window Classification"></a>Word Window Classification</h2><ul>
<li>任务目标: Classify a word in its context window of neighboring words.<br>eg. 命名实体识别任务: 对单词是Person, location, organization还是none进行分类。</li>
<li>相比于单个单词的分类，Word Window Classification是将上下文单词也进行考虑，从而帮助对中心词的分类。</li>
<li>与上文的单词分类相比，唯一改变的只是输入\(x_i\)，原来是一个单词的词向量，现在输入的是整个window所有单词词向量的拼接(concatenation)。通过整个window的context words来帮助对中心词的分类。</li>
<li>注：window模型有很多种处理方式，如可以将window中的所有单词不考虑位置作乱序处理，以及其他方法来更多地探索上下文信息。</li>
</ul>
<h2 id="神经网络分类"><a href="#神经网络分类" class="headerlink" title="神经网络分类"></a>神经网络分类</h2><ul>
<li>只用SoftMax分类的问题：只能提供线性分类的结果，在复杂数据集上的表现不好。<br><img src="/images/cs224n/4_Softmax classification2.png" width="400" height="350" alt="Softmax Classification Result" align="center"></li>
<li>神经网络的优点是具有强大的非线性拟合能力，而这种拟合能力很大程度上来源于神经元各种各样的非线性激活函数。<br><img src="/images/cs224n/4_nn activation function.png" width="400" height="350" alt="Why activation functions are needed" align="center"></li>
<li>Max-margin Loss function<br>依旧是ner的任务，对单词是不是地点进行分类。<br>s = score(museums in Paris are amzing)：中心词是地点的句子<br>sc = score(Not all meseums in Paris)：中心词不是地点的句子<br><strong>公式：</strong>$$ J = max(0, 1-s+sc) $$<br>idea：将中心词是地点的句子（正例）的得分最大化，中心词不是地点的句子（负例）的得分最小化。即正例句子的分值要比负例句子的分值 &gt; 1。<br>注1：将\(J\)值最小化即可 -&gt; 0。<br>注2：公式中的1可看作一个超参数，通常是1，也可是其他数。<br>注3：公式是连续的，因此可使用SGD。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://yoursite.com/2019/01/27/cs224n/lecture 4/" data-id="cjrziv6000032nkqx0cxsz399" class="article-share-link" data-share="baidu" data-title="lecture 4_Word Window Classification and Neural Networks">Share</a>
      

      
        <a href="http://yoursite.com/2019/01/27/cs224n/lecture 4/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cs224n/">cs224n</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Python/numpy笔记" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/25/Python/numpy笔记/" class="article-date">
  <time datetime="2018-12-25T17:11:34.447Z" itemprop="datePublished">2018-12-25</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/25/Python/numpy笔记/">numpy笔记</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <ul>
<li><p>基本类型: np.array<br>一维: <code>np.array([1,2])</code><br>二维: <code>np.array([[1,2], [3,4]])</code><br>三维: <code>np.array([[[1,2], [3,4]], [[5,6], [7,8]]])</code><br>四维: <code>np.array([[[[1,2], [3,4]], [[5,6], [7,8]]], [[[1,2], [3,4]], [[5,6], [7,8]]]])</code><br>把维度的增加理解为向高一级的抽象(四维数组就是多个三维数组的组合).</p>
</li>
<li><p>broadcasting<br>eg: <code>2*[1,2,3] = [2, 4, 6]</code><br>一种节省内存的方法, 让系统自动推断矩阵的shape(不用定义两个完全一样的shape, 相同的元素可以由向量省略为一个数)<br>详细文档: <a href="https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html" target="_blank" rel="noopener">https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html</a></p>
</li>
<li><p>行向量-&gt;列向量:<br>普通矩阵的转置直接<code>.T</code>即可, 但<code>.T</code>不适用行向量&amp;列向量, 因此需要其他方法: <code>reshape(-1, 1)</code>,只指定列宽为1, -1表示让系统自动推断大小.</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://yoursite.com/2018/12/25/Python/numpy笔记/" data-id="cjrziv6020036nkqxn27iluxn" class="article-share-link" data-share="baidu" data-title="numpy笔记">Share</a>
      

      
        <a href="http://yoursite.com/2018/12/25/Python/numpy笔记/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-cs224n/lecture 3" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/26/cs224n/lecture 3/" class="article-date">
  <time datetime="2018-11-26T22:55:12.828Z" itemprop="datePublished">2018-11-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/11/26/cs224n/lecture 3/">lecture 3_More Word Vectors</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="单词的向量表征"><a href="#单词的向量表征" class="headerlink" title="单词的向量表征"></a>单词的向量表征</h2><p>本节lecture主要是对上一节内容的扩展，重点还是在如何用向量来表征单词上，以使得单词间获得更好的相似性，而且我们可以直观地理解这种相似性。<br>核心思想是通过计算单词间的<strong>共现</strong>（co-occurence）次数/频率/概率来表征单词间的相似性——两个单词同时出现在相同context中的次数多了，我们就认为这两个单词比较相似。</p>
<ul>
<li>一种模型是skip-gram模型，一步步地计算每个窗口，最终得出单词间相似性。</li>
<li>另一种模型是Glove，基于共现矩阵的思想，一次性统计语料库中所有单词两两间的共现次数，从而得出单词间的相似性。</li>
</ul>
<h2 id="如何评估词向量的优劣"><a href="#如何评估词向量的优劣" class="headerlink" title="如何评估词向量的优劣"></a>如何评估词向量的优劣</h2><p>两种评价思路：intrinsic &amp; extrinsic（内部评价和外部评价）</p>
<ul>
<li><p>intrinsic evaluation<br>通过分析词向量本身的一些特性来评价：如词与词之间的余弦相似度、欧式距离或类比关系（analogy, 即 man-woman=king-queen 这种线性关系），如果得出的词向量这些关系好（eg. 词义相似的词向量在向量空间中距离也很近），那么这个词向量就好。<br>注：总的来说<strong>Glove</strong>表现的最好。</p>
</li>
<li><p>extrinsic evaluation<br>通过一些下游的任务来评价词向量的好坏，如我们把训练好的词向量用于命名实体识别任务中，看词向量实际表现的怎么样。<br>注：还是<strong>Glove</strong>模型表现的最好。</p>
</li>
<li><p>两种评价方法的优劣<br>intrinsic evaluation速度快，可以帮助我们更好地理解这个任务本身。但实际上这并不是一种真实的评价方法，即我们不知道这种词向量模型在实际任务中到底表现得怎么样。<br>extrinsic evaluation则是一种真实的评价方法，可以确切地得出词向量模型的好坏。但缺点是计算出最终结果很费时间，而且如果表现不好，我们并不能准确地分析出原因（到底是向量模型本身的原因，还是向量模型和其他模型互相影响(interact)的原因）</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://yoursite.com/2018/11/26/cs224n/lecture 3/" data-id="cjrziv5zt002rnkqxhoe8jg7y" class="article-share-link" data-share="baidu" data-title="lecture 3_More Word Vectors">Share</a>
      

      
        <a href="http://yoursite.com/2018/11/26/cs224n/lecture 3/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cs224n/">cs224n</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-cs224n/background" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/06/cs224n/background/" class="article-date">
  <time datetime="2018-11-06T21:07:19.846Z" itemprop="datePublished">2018-11-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/11/06/cs224n/background/">基础知识</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="梯度下降（gradient-decent）"><a href="#梯度下降（gradient-decent）" class="headerlink" title="梯度下降（gradient decent）"></a>梯度下降（gradient decent）</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script></p>
<ul>
<li>什么是梯度：在多元函数中，梯度可理解为求各个变量的偏导数，最终各个偏导组合成梯度向量，即代表该函数在该点变化最快的方向。<br>eg: \(f(x,y)=x^2+y^2\), 梯度向量为(2x,2y)，在点(1,1)处的梯度即为(2,2)，即沿(2,2)这个方向函数变化最快。</li>
<li>梯度下降的作用: 求损失函数的最小值. 因为梯度是函数值下降最快的方向, 所以经常用梯度来构造参数的更新表达式, 以此来求使得损失函数的函数值最小的参数值.</li>
</ul>
<h3 id="Vanilla-Gradient-Descent（普通梯度下降）"><a href="#Vanilla-Gradient-Descent（普通梯度下降）" class="headerlink" title="Vanilla Gradient Descent（普通梯度下降）"></a>Vanilla Gradient Descent（普通梯度下降）</h3><p>1、求解梯度向量<br>2、一点点沿着梯度的方向迭代更新函数值，使函数最终下降到局部最小值处。<br>$$<br>\theta^{new}_j=\theta^{old}_j-\alpha\nabla_{\theta}J(\theta)=\theta^{old}_j-\alpha\frac {\partial}{\partial \theta^{old}_j}J(\theta)<br>$$<br>注1: \(J(\theta)\)是损失函数, \(\theta\)是参数, \(\alpha\)是步长（step size).<br>注2: 步长\(\alpha\)的值若取太小则求解速度慢，取则会造成抖动。<br>解决方法：距离谷底较远时，步幅大些比较好（加快速度）；接近谷底时，步幅小些比较好（以免跨过界）。距离谷底的远近可以通过梯度的数值大小间接反映，接近谷底时，坡度会减小，因此可设置步长与梯度数值大小正相关。</p>
<h3 id="Stochastic-Gradient-Descent（SGD，随机梯度下降）"><a href="#Stochastic-Gradient-Descent（SGD，随机梯度下降）" class="headerlink" title="Stochastic Gradient Descent（SGD，随机梯度下降）"></a>Stochastic Gradient Descent（SGD，随机梯度下降）</h3><p>VGD中，损失函数相当于每个数据样本取平均值，因此每次更新都需要遍历所有data，当数据量太大，更新一次梯度会花费大量时间，因此并不可行。<br>解决这个问题的基本思路：只通过一个随机选取的数据(xn,yn)来获取梯度（通常损失函数都是很多项的变量加和得到的，这时只取一项计算其梯度，用来估计整体的梯度），这种方法叫随即梯度下降。<br>虽然这样估计梯度非常粗糙，但事实证明这种方法效果还不错。</p>
<h2 id="Softmax函数"><a href="#Softmax函数" class="headerlink" title="Softmax函数"></a>Softmax函数</h2><ul>
<li>作用：Takes an un-normalized vector, and normalizes it into a probability distribution. After applying Softmax, each element \(x_i\) will be in the interval \([0,1]\) and \(\sum_ix_i=1\).</li>
<li>公式：<br>$$<br>p(x_i) = \frac {exp(e_i)} {\sum_{n=1}^Nexp(x_n) }<br>$$</li>
<li>应用：可用于分类任务中，对目标进行线性分类；也经常用在神经网络中，用于将输出值归一化（原先值可能有负数，可能大于1，且所有值加和不为1），可视为产生概率分布。</li>
</ul>
<h2 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h2>
      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://yoursite.com/2018/11/06/cs224n/background/" data-id="cjrziv5zv002unkqxvyq85h8m" class="article-share-link" data-share="baidu" data-title="基础知识">Share</a>
      

      
        <a href="http://yoursite.com/2018/11/06/cs224n/background/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cs224n/">cs224n</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-cs224n/lecture 2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/05/cs224n/lecture 2/" class="article-date">
  <time datetime="2018-11-05T21:32:18.948Z" itemprop="datePublished">2018-11-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/11/05/cs224n/lecture 2/">lecture 2_Word Vectors</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script>

<h2 id="How-to-represent-the-meaning-of-a-word-in-computer"><a href="#How-to-represent-the-meaning-of-a-word-in-computer" class="headerlink" title="How to represent the meaning of a word in computer"></a>How to represent the meaning of a word in computer</h2><h3 id="离散表示（discrete-representation）"><a href="#离散表示（discrete-representation）" class="headerlink" title="离散表示（discrete representation）"></a>离散表示（discrete representation）</h3><ul>
<li>把单词作为一个个原子符号（atomic symbol）来表征：hotel, conference, walk…</li>
<li>对应到计算机中，使用<strong>独热编码（one-hot encoding）</strong>来存储：<code>[0 0 0 0 1 0 0 0]</code></li>
<li>存在的问题：<br>1、向量规模过大：每个单词都对应一个单独的位，若要存储所有单词，则会需要一个非常大的向量。<br>2、难以计算单词间的相似性：这是一种localist representation, 每个one-hot representation是独立存在的，no inherent notion of similarity, 两两间并没有天然的相似性关系。<br>3、因此与其去研究an approach to work out similarity relationship between one-hot representations，不如直接探究<strong>an approach where representation of word encodes its meaning inherently</strong>，这样就能直观去计算单词间的similarity，这就是分布表示（distributed representation）的设计初衷。<br>注：这里的similarity指的是单词词义上的相似，而不是结构相似。</li>
</ul>
<h3 id="分布表示（distributed-representation）"><a href="#分布表示（distributed-representation）" class="headerlink" title="分布表示（distributed representation）"></a>分布表示（distributed representation）</h3><ul>
<li><strong>核心思想：</strong>通过单词的上下文（context）来理解词意。如<code>banking</code>这个单词，我们让计算机知道经常和它一起出现的其他单词，就相当于明白了单词的用法——怎样将单词放到正确的上下文中，就相当于理解了单词的meaning。<br><img src="/images/cs224n/2_distributed representation.png" width="500" height="120" align="center"></li>
<li><strong>具体方法：</strong>用向量定义词语的含义。通过调整一个单词及其上下文单词的向量，使得根据两个向量可以推测两个单词词义的相似度，就可以根据中心词向量预测上下文/根据上下文向量预测中心词。这就是我们常说的<strong>word2vec</strong><br>遵循的基本原则：出现在相同上下文中的单词词义会相似。</li>
<li>注：区分distributed和distributional<br>distributed meaning：一种词义表示，和one-hot相对，one-hot将词义独立地存储在本地（<code>[0 0 0 0 1 0 0 0]</code>向量中），distributed则存储在一个大的稠密的向量空间中。<br>distributional similarity：一种通过上下文来理解词义的方法，与denotational相对。<br>We use distributional similarity to <strong>build</strong> distributed meaning.</li>
</ul>
<h2 id="word2vec模型"><a href="#word2vec模型" class="headerlink" title="word2vec模型"></a>word2vec模型</h2><h3 id="Skip-grams-SG"><a href="#Skip-grams-SG" class="headerlink" title="Skip-grams(SG)"></a>Skip-grams(SG)</h3><p><img src="/images/cs224n/2_Skipgram_prediction.png" width="300" height="150" alt="Skipgram_prediction" align="center/"><br><img src="/images/cs224n/2_Skipgram_model.png" width="450" height="300" alt="Skipgram_model" align="center/"></p>
<h4 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h4><p>通过中心词预测上下文。</p>
<h4 id="定义预测单词上下文的模型"><a href="#定义预测单词上下文的模型" class="headerlink" title="定义预测单词上下文的模型"></a>定义预测单词上下文的模型</h4><p><strong>预测模型：</strong>\(p(context|w_t)\)，表示在给定中心词\(w_t\)的条件下，正确预测出上下文单词的概率。<br><strong>再具体点：</strong>\(p(w_{t-1}|w_t)\)，\(p(w_{t-2}|w_t)\)，\(p(w_{t+1}|w_t)\)，\(t, t+1, t-1\)都是表示单词在文中出现的序号。<br><strong>最终形式：</strong><br>$$ p(w_{t+j}|w_t)=p(o|c)=\frac {exp(u_0^Tv_c)} {\sum_{w=1}^v exp(u_w^Tv_c)} $$<br>\(t, t+j\)是单词在文中的位置.<br>\(o, c\)则是单词在单词表中的序号，相当于在更为一般地表征两个单词的关联（与具体文本无关）。<br>\(v_c\)是中心词的词向量，\(u_0\)是上下文单词的词向量。<br><strong>公式理解：</strong>首先是\(u_0^Tv_c\)，这是一个点积操作，可用来粗糙地衡量单词间相似性（单词越相似，向量值越相近，乘积越大）；其次是softmax操作，用来把值转换成概率：<br>$$\frac {exp(u_0^Tv_c)} {\sum_{w=1}^v exp(u_w^Tv_c)} $$<br>最终即得出了由中心词预测上下文单词的概率。</p>
<h4 id="定义损失函数（Loss-Function）"><a href="#定义损失函数（Loss-Function）" class="headerlink" title="定义损失函数（Loss Function）"></a>定义损失函数（Loss Function）</h4><p>1、首先对模型全部相乘，表示文本整体的预测正确率：<br>$$ J^`(\theta)=\prod_{t=1}^T \prod_<br>{ \begin{align}<br>-m\le j \le m\\<br>j \neq 0<br>\end{align} }<br>p(w_{t+j}|w_t)$$<br>目标：整体正确率最大，maximize the function，<br>2、做Negtive Log Likelihood处理，使maximize-&gt;minimize，即得到最终的损失函数。<br>$$ J(\theta)=-\frac {1}{T} \sum_{t=1}^T \sum_<br>{ \begin{align}<br>-m&amp; \le j \le m\\<br>j \neq 0<br>\end{align} }<br>p(w_{t+j}|w_t)\\<br>p(w_{t+j}|w_t)=\frac {exp(u_0^Tv_c)} {\sum_{w=1}^v exp(u_w^Tv_c)}<br>$$<br>最终目标：使损失函数最小。</p>
<h4 id="怎样求损失函数最小值：梯度下降（gradient-descent）"><a href="#怎样求损失函数最小值：梯度下降（gradient-descent）" class="headerlink" title="怎样求损失函数最小值：梯度下降（gradient descent）"></a>怎样求损失函数最小值：梯度下降（gradient descent）</h4><p>注1：具体求解过程见lecture slide。<br>注2：求梯度时对\(u_o和v_c\)都要求偏导，最后组合成梯度向量。且求的是向量的导数，和实数求偏导有一定区别。<br>注3：当数据量过大时，可使用SGD随机梯度下降来加快速度（一个窗口计算一次梯度，用来估计整体梯度）。</p>
<h4 id="怎样训练模型"><a href="#怎样训练模型" class="headerlink" title="怎样训练模型"></a>怎样训练模型</h4><p>模型中的参数就是每个单词对应的word vector向量，将所有参数组合成一个的大向量\(\theta\)，即作为损失函数的自变量。<br><img src="/images/cs224n/2_theta.png" width="150" height="150" alt="theta举例（里面单词是随便举的例子）" align="center"><br>总共V个单词，每个单词对应两个词向量，词向量每个d维，因此\(\theta\)总长度为2dV。<br>为何每个单词对应两个词向量：作为中心词时一个，作为上下文单词时一个，这样更方便数学处理。</p>
<h3 id="Continuous-Bag-of-Words-CBOW"><a href="#Continuous-Bag-of-Words-CBOW" class="headerlink" title="Continuous Bag of Words(CBOW)"></a>Continuous Bag of Words(CBOW)</h3><h4 id="作用-1"><a href="#作用-1" class="headerlink" title="作用"></a>作用</h4><p>通过上下文预测中心词<br>To be continued</p>
<h2 id="遗留问题："><a href="#遗留问题：" class="headerlink" title="遗留问题："></a>遗留问题：</h2><ol>
<li>SG模型中，计算每个上下文词向量的值时，context matrix是一样的？ 那计算出的结果不就一样了？————context matrix是一样的，但因为每个单词的词向量表征是不一样的，所以最终训练出来的上下文词向量也是不一样的。直观来说，context martrix是一各由中心词-&gt;上下文的转换, 在词向量空间中从一个单词向这个单词周围来扩展(词义相近单词离得近), 这种变化对每个单词都是一样的, 因此context martrix一样也是正常的.</li>
<li>如何迭代\(\theta\)来更新梯度？把\(\theta\)初始化成什么样子？<br>————按照梯度下降的法则来更新参数; 参数初始化有多种策略.</li>
<li>这是有监督的学习过程，损失函数中没有出现计算误差的部分？</li>
</ol>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://yoursite.com/2018/11/05/cs224n/lecture 2/" data-id="cjrziv5zy002ynkqxa0h2zyt6" class="article-share-link" data-share="baidu" data-title="lecture 2_Word Vectors">Share</a>
      

      
        <a href="http://yoursite.com/2018/11/05/cs224n/lecture 2/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cs224n/">cs224n</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-cs224n/lecture 1" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/02/cs224n/lecture 1/" class="article-date">
  <time datetime="2018-11-02T12:33:38.275Z" itemprop="datePublished">2018-11-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/11/02/cs224n/lecture 1/">lecture 1_Introduction</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="关于人类语言-human-language"><a href="#关于人类语言-human-language" class="headerlink" title="关于人类语言(human language)"></a>关于人类语言(human language)</h2><h3 id="人类语言的特点"><a href="#人类语言的特点" class="headerlink" title="人类语言的特点"></a>人类语言的特点</h3><ul>
<li><strong>语言就是符号：</strong>人类语言本质是一个符号系统（symbol system），无论是汉字还是英文字母，都是一种符号，用来承载、传递我们想要表达的意思（meaning）。</li>
<li><strong>语言的载体：</strong>sound, vision(writting), gesture，不论是哪一种载体，都是一种连续的交流方式。</li>
<li><strong>大脑是一种符号处理器</strong>（symbolic processors）：我们可以把大脑处理语言看成是<strong>连续模式的激活过程</strong>（continious pattern of activation）。</li>
<li>因此我们可以得到启发：探索一种<strong>连续的编码模式</strong>来表达思想(explore a continous encoding patten of thought)。这也是很多NLP算法的处理思想，同时也解决了sparsity的问题。</li>
</ul>
<h2 id="关于NLP"><a href="#关于NLP" class="headerlink" title="关于NLP"></a>关于NLP</h2><h3 id="NLP-levels"><a href="#NLP-levels" class="headerlink" title="NLP levels"></a>NLP levels</h3><p><img src="/images/cs224n/1_NLP_levels.png" width="500" height="270" alt="NLP levels" align="center"></p>
<ul>
<li><strong>两大来源：</strong>通过语音或者文本。语音：语音分析（phonetic）或音韵分析（phonological）；文本：OCR识别（Optical Character Recognition，光学字符识别）或分词处理（tokenization）。通过上述方法来获取NLP的输入。</li>
<li><strong>形态分析</strong>（morphological）：对单词进行形态分析：前缀（prefix）、后缀（suffix）等。</li>
<li><strong>句法分析</strong>（syntactic）：分析句子结构、语法结构（structure of sentence）。</li>
<li><strong>语义理解</strong>（semantic interpretation）：work out the meaning of sentences.</li>
<li><strong>语篇处理</strong>（discourse processing）：因为大多数句子含义需要通过上下文（context）来推测，不能仅仅只分析当前句子，因此就有了the field of discourse processing。<br>注：cs224n课只重点讲syntatic &amp; semantic analysis 这两块，以及一部分speech signal analysis。</li>
</ul>
<h3 id="NLP-Applications"><a href="#NLP-Applications" class="headerlink" title="NLP Applications"></a>NLP Applications</h3><ul>
<li>较低级：spell checking, keyword search, finding synonyms</li>
<li>中级：extracting information。个人比较感兴趣的方向，让计算机可以阅读文本，理解在讲些什么，至少知道讲的是哪方面内容；从文本中识别、抽取某方面内容；或者为文本阅读难度分级（work out the reading level of school text）,识别文本的目标受众（intended audience of document）；情感分析（positive or negetive）。</li>
<li>高级：机器翻译、对话机器人、智能问答、机器撰写（exploit the knowledge of world）</li>
</ul>
<h3 id="Why-is-NLP-hard"><a href="#Why-is-NLP-hard" class="headerlink" title="Why is NLP hard"></a>Why is NLP hard</h3><ul>
<li><strong>语言本身的困难性</strong>：Ambiguilty of language, and moreover, humen always do not say everything（为了高效表达，语言使用中会出现很多省略）.</li>
<li><strong>表征语言很困难</strong>：Complexity of representing, using linguistic/situational/world knowledge.</li>
<li><strong>解释语言很困难</strong>：Real meaning of the language depends on real world, common sense, and contextual knowledge.</li>
</ul>
<h2 id="关于deep-learning"><a href="#关于deep-learning" class="headerlink" title="关于deep learning"></a>关于deep learning</h2><h3 id="传统机器学习的问题"><a href="#传统机器学习的问题" class="headerlink" title="传统机器学习的问题"></a>传统机器学习的问题</h3><ul>
<li>Most traditional machine learning algorithms work well because of human-designed representations and input featured.</li>
<li>“Machines” are only used to optimize weights that best make a final prdiction. </li>
<li>Moreover, manually designed featured are often over-specified(lack of generalization), incomplete and take a long time to design and validate.</li>
</ul>
<h3 id="What-is-deep-learning"><a href="#What-is-deep-learning" class="headerlink" title="What is deep learning"></a>What is deep learning</h3><ul>
<li>Subfield of machine learning and part of representation learning.</li>
<li>Deep learning algorithms attempt to learn (multiple levels of) representations and an output themselves. </li>
<li>We only input the raw data.</li>
<li>In a lot of times, deep learning means neural networks (the dominant model family).</li>
</ul>
<h3 id="deep-learning-in-NLP"><a href="#deep-learning-in-NLP" class="headerlink" title="deep learning in NLP"></a>deep learning in NLP</h3><p>核心思想：用vector去表征语言，用神经网络去组织、计算vector。</p>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://yoursite.com/2018/11/02/cs224n/lecture 1/" data-id="cjrziv5zx002wnkqxs1t1921e" class="article-share-link" data-share="baidu" data-title="lecture 1_Introduction">Share</a>
      

      
        <a href="http://yoursite.com/2018/11/02/cs224n/lecture 1/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cs224n/">cs224n</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-uwtsd_modules/Distributed &amp; Cluster Computing" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/10/18/uwtsd_modules/Distributed & Cluster Computing/" class="article-date">
  <time datetime="2018-10-18T21:29:42.408Z" itemprop="datePublished">2018-10-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/10/18/uwtsd_modules/Distributed & Cluster Computing/">Distributed &amp; Cluster Computing</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="Interfaces"><a href="#Interfaces" class="headerlink" title="Interfaces"></a>Interfaces</h2><ul>
<li>Interface-Based Programming<br>An interface only defines a signature of properties and methods(method name, type of each parameter, type of return value).<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">interface</span> <span class="title">IMyInterface</span></span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> property1</span><br><span class="line">    &#123;</span><br><span class="line">        get;</span><br><span class="line">        set;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Method1</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Method2</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>A class is used to implement the interface by providing the actual code for those methods.</p>
<ul>
<li>Interface Name Form<br>Microsoft dictates that all interface names start with the <code>I</code> characters, that they not include underscore character, and that they use Pascal casing when the name contains mutiple words(first letter of each word is uppercase). </li>
</ul>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://yoursite.com/2018/10/18/uwtsd_modules/Distributed & Cluster Computing/" data-id="cjrziv5zz0030nkqxwxt7wqgo" class="article-share-link" data-share="baidu" data-title="Distributed &amp; Cluster Computing">Share</a>
      

      
        <a href="http://yoursite.com/2018/10/18/uwtsd_modules/Distributed & Cluster Computing/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/uwtsd-modules/">uwtsd_modules</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Paper Notes/Efficient Estimation of Word Representations in Vector Space" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/09/13/Paper Notes/Efficient Estimation of Word Representations in Vector Space/" class="article-date">
  <time datetime="2018-09-13T07:49:59.000Z" itemprop="datePublished">2018-09-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/09/13/Paper Notes/Efficient Estimation of Word Representations in Vector Space/">Notes of Efficient Estimation of Word Representations in Vector Space读书笔记</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p><a href="https://arxiv.org/pdf/1301.3781.pdf?" target="_blank" rel="noopener">论文地址点这里</a></p>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script> 

<h2 id="论文内容概述"><a href="#论文内容概述" class="headerlink" title="论文内容概述"></a>论文内容概述</h2><p>这是谷歌发布的关于词向量的经典论文，针对传统模型无法表征单词间关联性的问题，本文提出了两种连续的词向量模型，并针对模型实现了分布式学习模块，最后基于词向量的线性运算设计了一种评价模型精度的方法，取得了良好的效果。</p>
<h3 id="简介-INTRODUCTION"><a href="#简介-INTRODUCTION" class="headerlink" title="简介(INTRODUCTION)"></a>简介(INTRODUCTION)</h3><ul>
<li><p><strong>传统NLP技术的特点</strong><br>传统NLP技术主要是基于单词间没有相似性的思想，将词汇以单词表索引的形式来表示。这样做虽然有很多好处，如简单、健壮、使用大量数据训练的简单模型的效果好于使用少量数据训练的复杂模型等；但同样也有很多限制，如语料数据库没有足够的数据、无法表征单词间的关联等。因此作者认为原先的技术很难再取得突破，需要去发掘新的技术。</p>
</li>
<li><p><strong>论文主要工作</strong><br>1、从大量单词和短语数据中学习出高质量的词向量，并且要保持一个合适的向量维度。<br>2、使用了一种评估词向量精度的技术，期望不仅相似的单词可以趋向接近，同时单词间能具有更多维的相似度。<br>3、使用单词偏移技术对词向量做简单的代数运算，以更好地挖掘词向量间的相似性规则。<br>4、设计了一个综合测试集来探究句法和语义的规律性，并研究了词向量维度和训练数据量这两个因素对训练时间和模型精度的影响。</p>
</li>
</ul>
<h3 id="模型框架-Model-Architectures"><a href="#模型框架-Model-Architectures" class="headerlink" title="模型框架(Model Architectures)"></a>模型框架(Model Architectures)</h3><p>在本节中，作者使用了NNLM和RNNLM两种神经网络来学习连续的单词表示方法，并且对不同神经网络的训练复杂度进行了分析。</p>
<ul>
<li><p><strong>训练复杂度公式</strong><br>作者首先提出了分析训练复杂度的公式：\(O=E\times T\times Q\)，其中E为训练的趟数，T为训练集中单词的数量，Q为表征模型复杂度的变量，下文还会对Q具体分析。</p>
</li>
<li><p><strong>前馈神经网络语言模型(NNLM)</strong><br>基本结构：整个网络分为输入层、映射层、隐含层和输出层，输入层将前N个单词编码为1-of-V的向量，V为词表大小，这N个单词共享相同的映射矩阵，阵映射到映射层，之后再从映射层转换到隐含层，经过一系列计算后输出最终结果。<br>模型复杂度：\(Q=N\times D+N\times D\times H+H\times V\)，因为N是一个较小的量，因此\(N\times D\)也比较小；另外对单词表用二叉树做优化，可使得\(V\)下降到\(log_2(V)\)，这样\(H\times V\)也会减小，最终模型的复杂度主要就集中在\(N\times D\times H\)，即映射层-&gt;隐含层这部分。<br>注：对于单词表的表示，还可以继续用哈夫曼树做优化，减少高频单词的编码长度，从而进一步降低输出层的计算量。</p>
</li>
<li><p><strong>循环神经网络语言模型(RNNLM)</strong><br>模型特点：RNN最大的特点是其自连接性，即状态的更新不仅取决于输入，还取决于上一时刻自己本身的状态，这个特性就使得RNN具有了一种“短时”的记忆，可以更好地表示前后文本之间的关联。<br>模型复杂度：RNNLM中没有映射层，因此模型复杂度简化为\(Q=H\times H+H\times V\)。和上文同理，\(V\)可以优化为\(log_2(V)\)，因此模型的复杂度主要集中在\(H\times H\)。</p>
</li>
<li><p><strong>神经网络的平行训练模型</strong><br>为了提升在大规模数据集上训练模型的效率，作者在DistBelief框架的基础上实现了一个分布式计算模型，允许多个副本并行地学习训练，而且所有副本会同步进行梯度更新，作者在下文会分析模型的具体效果。  </p>
</li>
</ul>
<h3 id="新的对数线性模型-New-Log-linear-Models"><a href="#新的对数线性模型-New-Log-linear-Models" class="headerlink" title="新的对数线性模型(New Log-linear Models)"></a>新的对数线性模型(New Log-linear Models)</h3><p>在本节中，作者提出了两个连续词向量模型，希望能使用一种更简单的模型来训练更大量的数据，以通过增加训练量来提升训练效果。</p>
<ul>
<li><p><strong>连续词袋模型(Continuous Bag-of-Words Model)</strong><br>基本结构：模型与前馈NNLM比较相似，但是去掉了非线性隐含层，并且所有单词直接共享整个映射层，而非只是映射矩阵。通过这种方法，所有的单词被映射到同一位置，对这些向量进行平均后即为最终的映射结果。<br>模型特点：1、所有的单词最终都被映射到同一位置，因此单词的顺序就不再起作用了；2、为了对单词进行分类，模型会同时使用上下文的一部分单词进行计算；3、模型的作用是根据上下文来推测中间可能出现的单词；4、与传统词袋模型的不同在于其对于上下文的表示是连续。<br>模型复杂度：\(Q=N\times D+D\times log_2(V)\)</p>
</li>
<li><p><strong>连续Skip-gram模型(Continuous Skip-gram Model)</strong><br>模型特点：1、模型的结构类似上述CBOW模型，可以说是“镜像对称”的；2、Skip-gram模型的作用也和CBOW模型相反，其是根据中间的单词，来推测单词的上下文信息；3、考虑到大部分距离较远的单词之间的关联性都较小，因此作者减少了远距离单词的训练样本数量，以降低其权重。<br>模型复杂度：\(Q=C\times (D+D\times log_2(V))\)，C指单词最大化距离，使用模型时会随机选取\([1,C]\)中的一个值作为取词窗口的长度。  </p>
</li>
</ul>
<h3 id="结果分析-Results"><a href="#结果分析-Results" class="headerlink" title="结果分析(Results)"></a>结果分析(Results)</h3><ul>
<li><p><strong>核心思想</strong><br>考虑明显具有相同相似关系的两对单词，如”biggest”、”big”和”smallest”、”small”，计算\(X=D(biggest)-D(big)+D(small)\)，若词向量模型训练成功，应有\(X\)与\(D(smallest)\)是向量空间中距离最近的点。</p>
</li>
<li><p><strong>实验设计</strong><br>作者从句法和语义的角度列出若干相似的类别，分别在每个类别中加入若干单词对，将这些单词对随机两两组合，基于上述公式，计算是否是距离最近的点，若是，即为一次正确的实验结果，最终统计总体正确率来评价模型的优劣。</p>
</li>
<li><p><strong>提升模型精度</strong><br>作者研究了训练数据量和词向量维度两个因素对CBOW模型精度的影响，发现如果只单纯提升某一个因素，对模型精度的影响都是有限的，因此需要同时提升两个因素才能取得比较好的效果。</p>
</li>
<li><p><strong>各模型的对比</strong><br>对比1：在相同训练量和向量维度的条件下，作者对比了RNNLM、NNLM、CBOW和Skip-gram四种模型的精确度，由Table 3可知，CBOW和Skip-gram两种模型的表现明显好于RNNLM、NNLM模型。<br>对比2：作者研究了增大训练量和增大向量维度对CBOW和Skip-gram两种模型的影响，发现只将训练量增大两倍和只将向量维度增大两倍，取得的效果是差不多的，增加的训练时间也比较接近；同时可以发现，两倍数据训练一趟比单倍数据训练三趟的效果要好。<br>对比3：作者将文中的模型与已公布的其他向量模型做了对比，综合表现最好的依旧是Skip-gram。<br>对比4：作者使用了之前提到的分布式计算框架，将向量维度提升到了1000，使用谷歌新闻数据集进行训练，在这种情况下Skip-gram模型的精度依旧是最高的，而且达到了65.6%。<br>对比5：介绍了微软的一个挑战赛，给定挖去一个词的句子和五个候选词，求哪个词和句子最匹配。作者将句子填词问题反向转化为了由词去预测句子的问题，预测出原句的概率最大的那个词，就是所求的词。作者利用Skip-gram+RNNLMs的组合模型，取得了所有模型中最好的效果。</p>
</li>
</ul>
<h2 id="个人感悟"><a href="#个人感悟" class="headerlink" title="个人感悟"></a>个人感悟</h2><h3 id="关于连续词向量模型"><a href="#关于连续词向量模型" class="headerlink" title="关于连续词向量模型"></a>关于连续词向量模型</h3><ul>
<li>本文的工作主要围绕着连续词向量模型，因为传统的词向量表示是将单词视作一个个离散的符号，这样做的缺点是无法提供足够的信息来体现词语之间的关联，比如虽然Italy和Rome两个单词看起来一点也不像，但其实两者之间是有联系的(Rome是Italy的首都)，而如何研究出这种潜在的联系，便是本文的主要讨论内容。  </li>
<li>作者的思路是通过词向量来表征这种潜在的联系，首先使用了两种神经网络来生成词向量，但因为隐含层的计算开销过大，使得模型无法训练大量数据集，也无法提升词向量的维度；因此作者从简化模型的角度，提出CBOW和Skip-gram两种模型，并且使用了分布式训练框架，这使得模型能够针对大量数据集进行训练，从而获得更好的训练效果。  </li>
<li>训练出向量模型后，作者为了检测向量模型的效果，基于“相似度高的向量距离近”的思想，提出了一种检测算法，对向量进行代数运算并计算之间的距离，如果相似度高的向量计算出的距离确实小，就说明模型是合理的，作者即通过这个思路分析了各个模型的优劣。</li>
</ul>
<h3 id="关于词向量的线性运算"><a href="#关于词向量的线性运算" class="headerlink" title="关于词向量的线性运算"></a>关于词向量的线性运算</h3><ul>
<li>文中所用的检测词向量精度的方法是基于词向量的线性运算，即“意大利-罗马+巴黎=法国”这样的计算规则，通过这个算式，我们可以很明显地感受到词之间所具有的某种关联，利用这种关联就可以大大扩展词向量的应用范围。  </li>
<li>但具体为什么会有这种性质呢？我认为可能的原因：词向量的假设是基于上下文的分布来推导词义，而“意大利-罗马+巴黎=法国”可以转换为“意大利+巴黎=法国+罗马”，而只有意大利和巴黎共同的上下文与法国和罗马共同的上下文是相似的，才会有这样的相等关系。而仔细考虑一下，这两对词的上下文确实是有可能比较像的，比如一篇介绍欧洲国家的文章，这些词所在的语境肯定是很相近的，这也就是我们把这些词作为相似词的原因。</li>
</ul>
<h3 id="关于训练数据量"><a href="#关于训练数据量" class="headerlink" title="关于训练数据量"></a>关于训练数据量</h3><ul>
<li>本文还有一个比较重要的思想是：简化模型，提升学习效率，利用更大的训练量来求得更好的训练效果。作者一开始就是因为神经网络模型的计算量太大，因此舍弃了隐含层，简化模型，从而提出了CBOW和Skip-gram这两个模型。</li>
<li>这样做虽然可能会损失一部分隐含层所提升的精度，但计算开销大大降低了，结合分布式学习模型，可以极大地提升训练量和向量维度的上限，从而弥补因简化模型而损失的精度。实验结果证明了这样的方案是可行的，这也为我们今后如何改善模型精度提供了一个思路。</li>
</ul>
<h2 id="改进方案"><a href="#改进方案" class="headerlink" title="改进方案"></a>改进方案</h2><h3 id="关于CBOW模型的语义问题"><a href="#关于CBOW模型的语义问题" class="headerlink" title="关于CBOW模型的语义问题"></a>关于CBOW模型的语义问题</h3><ul>
<li>在CBOW模型中，所有的单词最终都会映射到同一位置，模型是不考虑单词顺序的。我认为这样做主要是因为CBOW模型的作用是从上下文中推测相关的单词，即模型关注的是文章中是否出现单词，并不关注单词出现的顺序，因此舍弃顺序也是合理的。  </li>
<li>但这样做也会有一定问题，即一些语句虽然单词组成一样，但语义却有明显不同。如“李丽是谁的姐姐”和“李丽的姐姐是谁”，这两句话的词袋模型是完全一致的，但如果不考虑语法结构，我们很难得出正确的结果。因此在这种情况下，我们可以将词袋模型和句法分析相结合，来求出句子的真正含义。</li>
</ul>
<h3 id="CBOW模型和Skip-gram模型与神经网络相结合"><a href="#CBOW模型和Skip-gram模型与神经网络相结合" class="headerlink" title="CBOW模型和Skip-gram模型与神经网络相结合"></a>CBOW模型和Skip-gram模型与神经网络相结合</h3><ul>
<li>文章最后提到神经网络词向量和其他技术的结合可能会有很好的效果，作者在微软的挑战赛中也通过Skip-gram+RNNLMs取得了很高的预测精度，说明这个结合思路是可行的。  </li>
<li>因为首先CBOW和Skip-gram都未考虑单词的顺序，会带来一定的语义问题；同时RNN模型本身具有自连接性质，这可以使其对前后的单词的关联性有更好的“记忆”，因此两者结合可能会有互补的效果。关于实现，可以将两者的词向量模型以一定权重进行组合，得出新的词向量，以探究其在相关问题中的表现效果。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://yoursite.com/2018/09/13/Paper Notes/Efficient Estimation of Word Representations in Vector Space/" data-id="cjrziv5zn002enkqx4ekkqr7t" class="article-share-link" data-share="baidu" data-title="Notes of Efficient Estimation of Word Representations in Vector Space读书笔记">Share</a>
      

      
        <a href="http://yoursite.com/2018/09/13/Paper Notes/Efficient Estimation of Word Representations in Vector Space/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Paper-Notes/">Paper Notes</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Paper Notes/Personal Recommendation Using Deep Recurrent Neural Networks in NetEase" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/09/13/Paper Notes/Personal Recommendation Using Deep Recurrent Neural Networks in NetEase/" class="article-date">
  <time datetime="2018-09-13T07:49:59.000Z" itemprop="datePublished">2018-09-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/09/13/Paper Notes/Personal Recommendation Using Deep Recurrent Neural Networks in NetEase/">Personal Recommendation Using Deep Recurrent Neural Networks in NetEase读书笔记</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p><a href="http://cfm.uestc.edu.cn/~zhangdongxiang/papers/ICDE16_industry_231.pdf" target="_blank" rel="noopener">论文地址点这里</a></p>
<p><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script>  </p>
<h2 id="论文内容概述"><a href="#论文内容概述" class="headerlink" title="论文内容概述"></a>论文内容概述</h2><p>本文结合RNN和FNN两种神经网络提出了一种新的个性推荐方法，希望解决传统的CF方法无法进行实时推荐的问题，最终在网易的考拉电商网站上取得了良好的效果。</p>
<h3 id="简介-INTRODUCTION"><a href="#简介-INTRODUCTION" class="headerlink" title="简介(INTRODUCTION)"></a>简介(INTRODUCTION)</h3><ul>
<li><p><strong>传统CF模型存在的问题</strong><br>因为是基于用户购物习惯的推荐，使用的是历史信息，未能利用用户当前的浏览历史，因此无法进行实时推荐。</p>
</li>
<li><p><strong>如何进行实时推荐</strong><br>首先要考虑访问电商网站的用户属性，包括基本属性(浏览器、IP地址、个人基本信息、购买历史等)和动态属性(用户所浏览页面的信息)，而基于后者，我们就可以猜测用户在本次访问中究竟想要购买什么，从而完成实时推荐。</p>
</li>
<li><p>之后作者对系统主要面临的挑战、所构建DRNN的特点和其他一些技术作了介绍，即完成了本节内容。</p>
</li>
</ul>
<h3 id="推荐模块概述-OVERVIEW-OF-RECOMMENDATION-MODULE"><a href="#推荐模块概述-OVERVIEW-OF-RECOMMENDATION-MODULE" class="headerlink" title="推荐模块概述(OVERVIEW OF RECOMMENDATION MODULE)"></a>推荐模块概述(OVERVIEW OF RECOMMENDATION MODULE)</h3><ul>
<li><p><strong>系统流程</strong><br>服务器首先接收用户请求并聚合为一个会话信息，之后将其输入到推荐系统中，经过RNN和FNN计算后输出推荐结果，并在页面中显示。</p>
</li>
<li><p><strong>数据格式</strong><br>数据收集：假设用户\(u_i\)访问网站，会生成日志文档\(D_j^l\)和会话文档\(D_i^s\)两种文档，两者关系为\(D_i^s=\{D_0^l,D_1^l,\cdots,D_{k-1}^l\}\)，即一个会话文档对应多个日志文档。<br>数据简化：又因为每个日志文档都可简化为一个URL地址\(p_j\)，因此可得\(D_i^s=\{p_0,p_1,\cdots,p_{n-1}\}\)。<br>最终的输入数据：访问网站的每个用户\(u_i\)都会对应一个\(D_i^s\)，即构成了神经网络的输入数据。</p>
</li>
</ul>
<h3 id="DRNN具体介绍-DEEP-RECURRENT-NEURAL-NETWORK"><a href="#DRNN具体介绍-DEEP-RECURRENT-NEURAL-NETWORK" class="headerlink" title="DRNN具体介绍(DEEP RECURRENT NEURAL NETWORK)"></a>DRNN具体介绍(DEEP RECURRENT NEURAL NETWORK)</h3><ul>
<li><p><strong>RNN的特点</strong><br>RNN即循环神经网络，相比其他神经网络最大的特点是：其考虑了前后两个状态之间的关联，可以更好地处理序列信息。在本文的场景中，一个session可抽象为一系列的网页序列，因此利用RNN来进行推荐直观上是非常合适的。</p>
</li>
<li><p><strong>基本RNN模型</strong><br>在单个隐藏层的RNN中，隐藏层节点除了的输入和输出外，还会有一个自连接环，可以根据时间来不断地更新它的值。<br>更新函数：\(a(i)=f(Ux(i)+Wa(i-1))\)，\(a(i)\)表示在状态\(i\)下的节点值，\(x(i)\)表示输入值，\(U,W\)为相应的转移矩阵，\(f(x)\)为激活函数。<br>公式理解：隐藏层节点每次更新除了会考虑输入值外，还会考虑该节点在前一状态下的值，因此RNN的结果可以反映时间序列的相应信息。</p>
</li>
<li><p><strong>有限状态的DRNN</strong><br>当RNN具有多个隐藏层时，即构成了DRNN。考虑DRNN中第\(i\)层的某个状态\(t\)，其不仅会连接同层的状态\(t+1\)，还会连接到第\(i+1\)层的状态\(t\)，即构成了新的更新函数：<br>$$ f(x)=\left\{<br>\begin{align}<br>&amp;f(W_ia_i(t-1)+Z_i(a_{i-1}(t)+b_i(t)))&amp;,&amp; i&gt;1 \\<br>&amp;f(W_ia_i(t-1)+Z_i(V_t+\theta (p_t)))&amp;,&amp; i=1<br>\end{align}<br>\right.<br>$$</p>
</li>
<li><p><strong>引入历史状态节点的DRNN</strong><br>问题背景：受限于内存，我们不可能保存用户所有产生的状态；但如果使用\(n\)状态的滑动窗口，则只能选择最新的\(n\)个数据训练模型，会降低预测精度。因此作者引入了历史状态节点(history state)的概念。<br>当用户访问的页面数\(x\)超过一定数量\(n\)时，我们将前\(x-n\)个状态组合起来作为历史状态节点，有<br>$$\bar{V}=\sum_{i=0}^{x-n}\varepsilon_iV_i$$<br>$$\varepsilon_i=\frac {\theta(p_i)} {\sum_{j=i}^{x-n}\theta(p_j)} $$<br>公式理解：根据用户在页面的停留时间对前\(x-n\)个页面作加权平均，近似表征用户的历史信息，是一个既在一定程度上保证了模型精度，计算开销又不至于太大的折中方案。</p>
</li>
<li><p><strong>与协同过滤算法的结合</strong><br>问题背景：虽然协同过滤算法无法提供实时的推荐，但如果用户遵循以往购买习惯，其推荐效果还是很好的。因此作者引入了FNN模型来模拟CF算法，作为RNN的补充。<br>另外使用FNN还有两点好处：</p>
<ul>
<li>FNN和RNN共享相同的输出层，因此可以将二者的输出融合起来作为最终结果，来表征用户购买某件商品的概率。</li>
<li>可以使用随机最速下降法(SGD)来训练RNN和FNN结合的权重，而不用人为地决定哪个网络更为重要，减轻了模型调参的工作量。</li>
</ul>
</li>
<li><p><strong>如何生成训练数据</strong><br>用户从进入网站开始到最终购买商品，会经历一定数量的页面，个性推荐的本质目标就是减少这之间页面的数量。对于一次购买行为\(I\)，其对应的页面路径为\(p_0,\cdots,p_{n-1}\rightarrow I\)。若对其进行优化，不一定非要优化成\(p_0\rightarrow I\)这样(当然这是最优情况)，只要能减少用户的页面访问数量，都可以算作优化。因此我们的训练数据还可以是\(p_0,p_1\rightarrow I\)、 \(p_0,p_1,p_2\rightarrow I\)等，这样一次购买行为就可以产生\(n\)组训练数据，大大增加了我们的训练量。</p>
</li>
<li><p><strong>模型的实现</strong><br>作者使用了Caffe框架来实现模型，整个网络包含三层隐含层，且同一层次的神经元共享相同的权重和偏置矩阵。此外，模型的RNN部分包含4个状态的输入，FNN包含1个状态的输入。</p>
</li>
</ul>
<h3 id="模型调优-MODEL-OPTIMIZATIONS"><a href="#模型调优-MODEL-OPTIMIZATIONS" class="headerlink" title="模型调优(MODEL OPTIMIZATIONS)"></a>模型调优(MODEL OPTIMIZATIONS)</h3><p>这一节作者介绍了其为了改进模型性能所做的工作，并提出了一种自动调优框架，使得模型具有了更高的精度和更快的学习速度。</p>
<ul>
<li><p><strong>自动代码生成器</strong><br>问题背景：模型中包括了很多参数，调整这些参数需要更改甚至重写Caffe脚本，非常繁琐。因此作者构建了一个代码生成器，其主要任务是接收参数值，输出对应的Caffe脚本。<br>主要思想：首先将参数分为<strong>基本参数</strong>(损失函数、学习速率等)和<strong>网络结构参数</strong>(每层的神经元数)，调整基本参数只需更改相应的值，而调整网络结构参数则需要改写Caffe脚本，因此我们只需重点关注网络结构参数即可。<br>三种网络结构参数：文中提出了长、宽、高三种网络结构参数，分别对应隐含层数量、状态数以及隐含层与状态层的连接数，并编写了相应的代码生成算法，具体可见文中的Algorithm 1。</p>
</li>
<li><p><strong>模型调优</strong><br>为了求得表现更好的模型参数，作者采用了遗传算法这种启发式算法进行模型调优。<br>染色体结构：\(C=(w,l,h,a_1,a_2,\cdots,a_L,\cdots)\)，直观的理解就是将所有参数结合在了一起。<br>适应度函数：\(fit=accuracy+\frac {1} {1+loss}\)，\(accuracy\)为模型预测精度，\(loss\)为损失率。<br>注：虽然遗传算法最终求得的是局部最优解，但因为参数调优本就是一个非常复杂、难以建模的过程，所以作者认为这样的解已经足够好了。</p>
</li>
</ul>
<h3 id="模型实验-EXPERIMENTS"><a href="#模型实验-EXPERIMENTS" class="headerlink" title="模型实验(EXPERIMENTS)"></a>模型实验(EXPERIMENTS)</h3><p>这一节作者对模型进行了全面的测试和分析，并分别研究几个重要因素对模型的影响。</p>
<ul>
<li><p><strong>评价指标</strong><br>作者采用了预测正确率作为模型主要的评价指标，公式为\(accuracy=\frac {f(S)} {|S|}\)，\(S\)代表训练样本总数，\(f(S)\)代表正确预测的样本数。</p>
</li>
<li><p><strong>batch size的影响</strong><br>在使用默认参数训练模型时，增大batch size可以提高模型精度，但对于内存的消耗也更大。而一个有趣的现象是，使用调优框架后再训练模型，batch size对精度的影响就不再显著了，可见调优框架确实使得模型的表现更为优异了。</p>
</li>
<li><p><strong>FNN的影响</strong><br>由Fig.11、Fig.12可得，FNN的使用显著提升了模型精度，尤其是同时使用调优框架和FNN的模型，精度提升了约10%，而且这还是在模型只推荐1个物品的情况下(即购买概率最大的那个物品)，若模型返回10个物品，模型精度可以达到50%以上。<br>同时，使用FNN并不会影响模型的收敛速率，即模型精度的提升并不会增加计算开销，这也是很重要的一点。</p>
</li>
<li><p><strong>历史状态节点的影响</strong><br>在使用默认参数训练模型时，使用历史状节点态可提高模型10%的精度；而使用了调优框架后，只能提升2%的精度，即调优过程降低了不同网络结构对模型的影响，这也侧面表明了作者所提出调优框架的优异性能。</p>
</li>
<li><p><strong>模型最终效果</strong><br>在DRNN和FNN结合的情况下，模型最终的预测精度达到了33.13%，页面路径压缩到了原先的72.41%，相当于用户从点进网站到最终购买商品，少浏览了30%的页面，效果还是显而易见的，毕竟用户每多浏览一个自己不感兴趣的页面，其离开网站的概率就会越大，这也就是推荐系统的作用所在，即帮助用户更快地进行选择。</p>
</li>
</ul>
<h2 id="个人感悟"><a href="#个人感悟" class="headerlink" title="个人感悟"></a>个人感悟</h2><h3 id="关于推荐系统"><a href="#关于推荐系统" class="headerlink" title="关于推荐系统"></a>关于推荐系统</h3><ul>
<li>本文提出了基于RNN和FNN的推荐系统，最主要的原因是传统的CF算法基于用户间购物历史的相似度来做推荐，考虑的更多是用户的购物习惯，这就使其无法把握用户突发的、低频的购买需求。</li>
<li>比如我平常很爱买衣服，网购的大部分物品都是衣服，但有一天突然想吃零食，就会在网站上浏览零食的相关页面，这时系统给我推荐的若还是衣服，就会影响用户体验，也不利于提高网站的销量，因此实时推荐是很有必要的。</li>
<li>但同时，在生活大部分情况中，一个人还是会遵循其所形成的习惯去购物，这时基于购物习惯的推荐便能表现出很好的效果，因此CF算法也是很有必要的。</li>
<li>综上，论文中使用RNN和FNN两种算法共同完成推荐，这种思路是合理的，也是符合我们的直观认知的。</li>
</ul>
<h3 id="关于RNN算法"><a href="#关于RNN算法" class="headerlink" title="关于RNN算法"></a>关于RNN算法</h3><ul>
<li>RNN与其他网络最大的不同在于其隐含层节点的自连接性，在其更新函数中，不仅包括正常的输入值，还包括上一时刻中节点自身的值，这就像使得节点具有了“记忆”，这个记忆表征了时间序列中前后节点的关联，因此RNN适合处理时间序列或状态间具有一定联系的情况。</li>
<li>对应到本文，用户在网站上购物必然会浏览一系列页面，而这些页面是否是有关联的呢？我认为是有的。比如我要买一个钱包，在第一个页面中没有浏览到我喜欢的款式，那我下一个访问的页面也会是关于钱包的，甚至我之后的浏览可能都会围绕钱包来展开，因此我们就可从用户初始的浏览内容来推测其实时的购买兴趣，这也就是作者应用RNN进行实时推荐的原因。</li>
</ul>
<h3 id="关于历史状态节点-history-state"><a href="#关于历史状态节点-history-state" class="headerlink" title="关于历史状态节点(history state)"></a>关于历史状态节点(history state)</h3><ul>
<li>本文的一个创新在于引入了历史状态节点，来解决状态数过多、计算开销过大的问题。根据用户在页面上停留的时间，将多出的历史状态作加权平均，构成一个新的节点，这样既可以保留历史信息，又不会造成计算开销的大量增加，是一种比较折衷的方案。</li>
<li>这也提示我们，在数据量过大的情况下，与其直接将一部分数据丢弃掉，不如将这部分数据做整合，采用加权平均或其他的提取信息的方法，构成新的节点来参与运算，在模型精度和计算开销之间取得平衡。</li>
</ul>
<h3 id="关于训练数据的生成"><a href="#关于训练数据的生成" class="headerlink" title="关于训练数据的生成"></a>关于训练数据的生成</h3><ul>
<li>本文针对用户一次的购买行为，将各种可能的路径优化方案都作为了训练数据送入模型进行训练，这样可以增加我们的训练量。因为对于有监督学习来说，带标签的数据是有限的，如何充分利用有限的标签数据去训练模型需要我们去研究，本文给了我们一种可行的思路。</li>
</ul>
<h3 id="关于模型调优"><a href="#关于模型调优" class="headerlink" title="关于模型调优"></a>关于模型调优</h3><ul>
<li>本文提出了一种模型调优框架，大致可分为“生成参数”和“生成代码”两部分。</li>
<li>首先将模型的相关参数整合成染色体，再利用遗传算法的杂交、变异等操作，生成下一代染色体，之后将参数传入代码生成器中生成对应的Caffe脚本，训练模型后，将结果回带到遗传算法的适应度函数中进行评估和自然选择，一轮轮地迭代，最终即可得出最优的参数组合。</li>
<li>因为建模的复杂性和组合爆炸等问题，模型调优一直是机器学习的一个难点，在这种情况下，利用一些启发式算法进行智能调优不失为一个好的方法，这也是文章给我们的一个启示。</li>
</ul>
<h2 id="改进方案"><a href="#改进方案" class="headerlink" title="改进方案"></a>改进方案</h2><h3 id="关于模型的更新问题"><a href="#关于模型的更新问题" class="headerlink" title="关于模型的更新问题"></a>关于模型的更新问题</h3><ul>
<li>在文章中提到，顾客每完成一次购物，就相当于得到了一个”ground truth”，可以用来训练、调整模型，这样模型随着网站的运行就会不断优化和改进。</li>
<li>如果顾客这次购物是在遵循自己以往的购物习惯，那么将结果用来继续训练模型是没有问题的，因为这个行为在今后还会多次发生，这样可以使得模型更了解顾客的购买习惯，从而更好地完成推荐。</li>
<li>但如果这是一次”unexpected”的购物行为，就像平常都喜欢买衣服的我只是突然想吃点零食，如果模型把这个突发的低频需求当作了用户的购买习惯，在今后也多次向用户进行推荐，可能会造成用户的厌烦。</li>
<li>因此我认为在模型的持续更新中，应该考虑到用户购买行为的属性，即对利用了RNN方式完成的推荐，要降低其对模型的后续影响。</li>
<li>方案实现：可以通过减少训练样本数量的方式，对于通过RNN推荐完成的购买，不要将<br>$$<br>  p_0 \rightarrow I \\<br>  p_0,p_1 \rightarrow I \\<br>  \cdots \\<br>  p_0,\cdots,p_{n-1} \rightarrow I<br>$$<br>所有的路径样本都送入训练，可以只送入后半段或后1/4的样本，让推荐发生的条件更为“苛刻”一些，从而使得模型不会在用户一进入网站就推荐一些低频、不经常需求的产品。</li>
</ul>
<h3 id="将社群属性加入到模型中"><a href="#将社群属性加入到模型中" class="headerlink" title="将社群属性加入到模型中"></a>将社群属性加入到模型中</h3><ul>
<li>论文在”Related Work”中提到CF推荐和基于内容的推荐相结合，会在社交网络上表现得更好，因此我认为可以将这种思路应用到本文，即在推荐系统中加入一定的社群属性。</li>
<li>问题背景：在实际生活中，我们会更倾向于接受来自朋友的推荐，而非来自商家的推荐。尤其是本文的电商网站——网易考拉，是一个主打海淘的平台，用户对一些国外的品牌可能了解并不多，这时如果有来自自己社交圈的推荐，无疑会增加购买的概率。</li>
<li>方案实现：可以考虑改进文中的FNN部分的方法，即首先通过用户填写的基本信息或其他网易系应用中的用户资料(如网易音乐、游戏等)，对用户进行社群判别和分类，为每个用户构建一个“熟人圈”。在进行FNN推荐时，模型不仅推荐历史相似的用户购买的产品，也推荐来自熟人圈购买的产品，以将社群属性加入到推荐模型中。</li>
</ul>
<h3 id="关于推荐理由"><a href="#关于推荐理由" class="headerlink" title="关于推荐理由"></a>关于推荐理由</h3><ul>
<li>问题背景：目前用户对于推荐系统的态度，不仅是想“知其然”，也想“知其所以然”，即除了推荐的物品本身，用户也会想知道系统为什么会给自己推荐这个物品，因此如果能给推荐物品附上推荐理由，无疑会提升网站的用户体验。</li>
<li>方案实现：因为本文是将两种神经网络的输出层共享，以类似加权平均的方式进行融合(见文章第三节D部分)，来计算出用户购买某件物品的概率的，因此可以将最终结果的各个部分分离出来，从大到小排序，通过分析值最大的一项或几项来构造我们的推荐理由。</li>
<li>具体形式：推荐理由的形式可以为“根据你以前购买过的xxx牛仔裤，我们猜你还喜欢这个”、“根据你刚刚浏览过的xxx钱包，我们猜你会喜欢这个”等等，以一种猜测、活泼的口吻对用户进行提示，以增加用户的购买欲望。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://yoursite.com/2018/09/13/Paper Notes/Personal Recommendation Using Deep Recurrent Neural Networks in NetEase/" data-id="cjrziv5zo002gnkqx3gaop21t" class="article-share-link" data-share="baidu" data-title="Personal Recommendation Using Deep Recurrent Neural Networks in NetEase读书笔记">Share</a>
      

      
        <a href="http://yoursite.com/2018/09/13/Paper Notes/Personal Recommendation Using Deep Recurrent Neural Networks in NetEase/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Paper-Notes/">Paper Notes</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
  </nav>
</section>
      
      <aside id="sidebar">
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithm/">Algorithm</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/C/">C++</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/C-Sharp/">C_Sharp</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/English/">English</a><span class="tag-list-count">8</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Git/">Git</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/JSP/">JSP</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Java/">Java</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lingo/">Lingo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MachineLearning/">MachineLearning</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Network/">Network</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Other/">Other</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Paper-Notes/">Paper Notes</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SQL/">SQL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Swift/">Swift</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cs224n/">cs224n</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nlp/">nlp</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/uwtsd-modules/">uwtsd_modules</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/课程笔记/">课程笔记</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Algorithm/" style="font-size: 12.86px;">Algorithm</a> <a href="/tags/C/" style="font-size: 11.43px;">C++</a> <a href="/tags/C-Sharp/" style="font-size: 10px;">C_Sharp</a> <a href="/tags/English/" style="font-size: 20px;">English</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/JSP/" style="font-size: 10px;">JSP</a> <a href="/tags/Java/" style="font-size: 10px;">Java</a> <a href="/tags/Lingo/" style="font-size: 10px;">Lingo</a> <a href="/tags/MachineLearning/" style="font-size: 17.14px;">MachineLearning</a> <a href="/tags/Network/" style="font-size: 15.71px;">Network</a> <a href="/tags/Other/" style="font-size: 18.57px;">Other</a> <a href="/tags/Paper-Notes/" style="font-size: 12.86px;">Paper Notes</a> <a href="/tags/SQL/" style="font-size: 10px;">SQL</a> <a href="/tags/Swift/" style="font-size: 10px;">Swift</a> <a href="/tags/cs224n/" style="font-size: 15.71px;">cs224n</a> <a href="/tags/nlp/" style="font-size: 10px;">nlp</a> <a href="/tags/python/" style="font-size: 14.29px;">python</a> <a href="/tags/uwtsd-modules/" style="font-size: 10px;">uwtsd_modules</a> <a href="/tags/课程笔记/" style="font-size: 10px;">课程笔记</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">June 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a><span class="archive-list-count">26</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/02/07/Regular Expression/">正则表达式</a>
          </li>
        
          <li>
            <a href="/2019/01/27/cs224n/lecture 4/">lecture 4_Word Window Classification and Neural Networks</a>
          </li>
        
          <li>
            <a href="/2018/12/25/Python/numpy笔记/">numpy笔记</a>
          </li>
        
          <li>
            <a href="/2018/11/26/cs224n/lecture 3/">lecture 3_More Word Vectors</a>
          </li>
        
          <li>
            <a href="/2018/11/06/cs224n/background/">基础知识</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Links</h3>
    <div class="widget">
      <ul>
        
      </ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Ren Li<br>
      Powered by <a href="//hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/xiangming/landscape-plus" target="_blank">Landscape-plus</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
  <!-- totop start -->
<div id="totop">
<a title="totop"><img src="/img/scrollup.png"/></a>
</div>

<!-- totop end -->

<!-- 多说公共js代码 start -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"reqianduan"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
     || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
  </script>
<!-- 多说公共js代码 end -->


<!-- 百度分享 start -->

<div id="article-share-box" class="article-share-box">
  <div id="bdshare" class="bdsharebuttonbox article-share-links">
    <a class="article-share-weibo" data-cmd="tsina" title="分享到新浪微博"></a>
    <a class="article-share-weixin" data-cmd="weixin" title="分享到微信"></a>
    <a class="article-share-qq" data-cmd="sqq" title="分享到QQ"></a>
    <a class="article-share-renren" data-cmd="renren" title="分享到人人网"></a>
    <a class="article-share-more" data-cmd="more" title="更多"></a>
  </div>
</div>
<script>
  function SetShareData(cmd, config) {
    if (shareDataTitle && shareDataUrl) {
      config.bdText = shareDataTitle;
      config.bdUrl = shareDataUrl;
    }
    return config;
  }
  window._bd_share_config={
    "common":{onBeforeClick: SetShareData},
    "share":{"bdCustomStyle":"/css/bdshare.css"}
  };
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?cdnversion='+~(-new Date()/36e5)];
</script>

<!-- 百度分享 end -->

<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>




<script src="/js/script.js"></script>

</div>
</body>
</html>
